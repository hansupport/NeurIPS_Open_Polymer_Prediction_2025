import os
import gc
import sys
import time
from pathlib import Path
from collections import defaultdict
import pandas as pd
from tqdm import tqdm
from tqdm.contrib import DummyTqdmFile


# âœ… ëª¨ë“  print ì¶œë ¥ì„ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥í•˜ê¸° ìœ„í•œ ë¡œê±° í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼)
class TeeLogger:
    def __init__(self, filename, mode='w'):
        self.file = open(filename, mode, encoding='utf-8')
        self.stdout = sys.stdout  # ì›ë˜ì˜ stdoutì„ ì €ì¥
        sys.stdout = self          # í˜„ì¬ stdoutì„ ì´ ê°ì²´ë¡œ ë³€ê²½

    def close(self):
        """
        ë¡œê±°ë¥¼ ì¢…ë£Œí•˜ê³  stdoutì„ ì›ë˜ëŒ€ë¡œ ë³µì›í•©ë‹ˆë‹¤.
        """
        if self.stdout:
            sys.stdout = self.stdout # ì €ì¥í•´ë‘ì—ˆë˜ ì›ë˜ stdoutìœ¼ë¡œ ë³µì›
            self.stdout = None # ì¤‘ë³µ ë³µì›ì„ ë§‰ê¸° ìœ„í•´ Noneìœ¼ë¡œ ì„¤ì •
        if self.file:
            self.file.close()
            self.file = None

    def __del__(self):
        self.close()

    def write(self, data):
        if self.file:
            self.file.write(data)
        if self.stdout:
            self.stdout.write(data)

    def flush(self):
        if self.file:
            self.file.flush()
        if self.stdout:
            self.stdout.flush()


# âœ… ì‹¤í—˜ ë¡œì§ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬
def run_single_experiment(params, output_dir):
    """
    ë‹¨ì¼ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    start_time = time.time()

    # Feature Engineering Flags
    DO_RDKit = params['DO_RDKit']
    DO_POLYBERT = params['DO_POLYBERT']
    DO_FINGERPRINT = params['DO_FINGERPRINT']
    RDKit_FILTER_MODE = params['RDKit_FILTER_MODE']

    # Augmentation Flags
    DO_AUGMENT_SMILES = params['DO_AUGMENT_SMILES']
    AUG_NUM = params['AUG_NUM']
    DO_GMM_AUGMENT = params['DO_GMM_AUGMENT']
    GMM_SAMPLES = params['GMM_SAMPLES']
    GMM_COMPONENTS = params['GMM_COMPONENTS']
    GMM_RANDOM_STATE = params['GMM_RANDOM_STATE']

    # Preprocessing Flags
    DO_VARIANCE_THRESHOLD = params['DO_VARIANCE_THRESHOLD']
    VARIANCE_THRESHOLD = params['VARIANCE_THRESHOLD']
    DO_StandardScaler = params['DO_StandardScaler']

    # Label & Model Flags
    N_SPLITS = params['N_SPLITS']

    # ì‹¤í—˜ ì‹œì‘ ë¡œê·¸
    print("=" * 50)
    print(f"Running experiment with settings:\n{params}")
    print("=" * 50)

    # =================================================================

    # 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
    import os
    import gc
    import sys
    import joblib
    import shutil
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from pathlib import Path
    from collections import defaultdict
    from tqdm import tqdm

    # ëª¨ë¸ ê´€ë ¨
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, Dataset
    import xgboost as xgb
    import lightgbm as lgb
    import catboost as cb
    from sklearn.ensemble import ExtraTreesRegressor
    from sklearn.model_selection import KFold
    from sklearn.metrics import mean_absolute_error
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    from sklearn.pipeline import Pipeline
    from sklearn.feature_selection import VarianceThreshold
    from sklearn.preprocessing import StandardScaler

    # ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
    import warnings
    warnings.filterwarnings('ignore')
    torch.autograd.set_detect_anomaly(True)

    # cpu gpu ì„¤ì •
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ------------------------------------------------------------
    # âš™ï¸ ì‚¬ìš©ì ì„¤ì • ì˜ì—­
    # ------------------------------------------------------------

    # ------------------------------------------------------------
    #
    # ì „ì²˜ë¦¬ ì‹¤í–‰ í”Œë˜ê·¸
    # Trueë¡œ ì„¤ì • ì‹œ, main() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒˆë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    # Falseë¡œ ì„¤ì • ì‹œ, ì „ì²˜ë¦¬ë¥¼ ê±´ë„ˆë›°ê³  INPUT_DIRì— ì§€ì •ëœ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    RUN_PREPROCESSING = True

    # ëª¨ë¸ ì‹¤í–‰ í”Œë˜ê·¸
    # Trueë¡œ ì„¤ì • ì‹œ, ëª¨ë¸ í›ˆë ¨ ë° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    # Falseë¡œ ì„¤ì • ì‹œ, ëª¨ë¸ í›ˆë ¨ ê³¼ì •ì„ ê±´ë„ˆë›°ê³  MODEL_INPUT_DIRì— ì§€ì •ëœ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    RUN_MODEL_TRAINING = True

    # ì œì¶œ ëª¨ë“œ í”Œë˜ê·¸
    # Trueë¡œ ì„¤ì • ì‹œ, í›ˆë ¨ê³¼ í•¨ê»˜, ì €ì¥ëœ ëª¨ë¸ë¡œ submission.csvë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    # Falseë¡œ ì„¤ì • ì‹œ ì œì¶œ
    SUBMISSION_MODE = False
    # ------------------------------------------------------------

    # ------------------------------------------------------------
    # Augmentation Flags 2
    # DO_GMM_AUGMENT = False  # GMMì„ ì´ìš©í•œ ë°ì´í„° ì¦ê°•
    # GMM_SAMPLES = 500  # GMMìœ¼ë¡œ ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
    # GMM_COMPONENTS = 5  # GMM ì»´í¬ë„ŒíŠ¸ ìˆ˜
    # GMM_RANDOM_STATE = 42  # GMM ì‹œë“œ
    # ------------------------------------------------------------

    # ------------------------------------------------------------
    # Normalization Flags
    # DO_VARIANCE_THRESHOLD = False  # Variance Threshold ì‹¤í–‰ ì—¬ë¶€
    # VARIANCE_THRESHOLD = 0.1  # ìµœì†Œ ë¶„ì‚° ì„ê³„ê°’

    # DO_StandardScaler = False  # StandardScaler ì‹¤í–‰ ì—¬ë¶€
    # ------------------------------------------------------------

    # ------------------------------------------------------------
    # ì „ì²˜ë¦¬ ì‹¤í–‰ í”Œë˜ê·¸ê°€ Trueì¼ ë•Œ, ì „ì²˜ë¦¬ ì‹¤í–‰í•  íƒ€ê²Ÿ
    PREPROCESS_TARGET = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']
    # ------------------------------------------------------------

    # 2. ê²½ë¡œ ì„¤ì •
    # ------------------------------------------------------------
    # 2-1. input
    # ------------------------------------------------------------
    # ì´ì „ ë…¸íŠ¸ë¶ì—ì„œ ìƒì„±ëœ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.
    # ì°¸ê³ : RUN_PREPROCESSING = Falseì¼ ë•Œ ì‚¬ìš©í•  ê¸°ë³¸ ê²½ë¡œì…ë‹ˆë‹¤.
    #        Trueì¼ ê²½ìš°, ì´ ê²½ë¡œëŠ” ì•„ë˜ ë¸”ë¡ì—ì„œ ë™ì ìœ¼ë¡œ ë³€ê²½ë©ë‹ˆë‹¤.
    PRE_INPUT_DIR = Path("Dataset/PreprocessingData")
    # ------------------------------------------------------------
    TEST_INPUT_DIR = OUTPUT_DIR / Path("features")
    SAMPLE_SUBMISSION = Path("Dataset/neurips-open-polymer-prediction-2025")

    # ì¶”ë¡ ë§Œ í•  ê²½ìš°
    MODEL_INPUT_DIR = Path("Dataset/ModelData")
    # ------------------------------------------------------------

    # ------------------------------------------------------------
    # 2-2. output
    # ------------------------------------------------------------
    SUBMISSION_SAVE_DIR = OUTPUT_DIR
    SUBMISSION_SAVE_DIR.mkdir(exist_ok=True, parents=True)

    MODEL_SAVE_DIR = OUTPUT_DIR / Path("models")
    MODEL_SAVE_DIR.mkdir(exist_ok=True, parents=True)

    GRAPH_SAVE_DIR = OUTPUT_DIR / Path("graphs")
    GRAPH_SAVE_DIR.mkdir(exist_ok=True, parents=True)

    FOLD_GRAPH_SAVE_DIR = OUTPUT_DIR / Path("graphs/folds")
    FOLD_GRAPH_SAVE_DIR.mkdir(exist_ok=True, parents=True)

    OOF_SAVE_DIR = OUTPUT_DIR / Path("oof")
    OOF_SAVE_DIR.mkdir(exist_ok=True, parents=True)

    IMPORTANCE_SAVE_DIR = OUTPUT_DIR / Path("importance")
    IMPORTANCE_SAVE_DIR.mkdir(exist_ok=True, parents=True)

    FTT_DETAIL_SAVE_DIR = OUTPUT_DIR / Path("FTTdetails")
    FTT_DETAIL_SAVE_DIR.mkdir(exist_ok=True, parents=True)
    # ------------------------------------------------------------

    # 3. K-Fold, FTT, ET ì„¤ì •
    # ------------------------------------------------------------
    # K-Fold ì„¤ì •
    # N_SPLITS = 2
    RANDOM_STATE = 42

    # FTT ì„¤ì •
    CATEGORICAL_THRESHOLD = 20

    # 'LGBM', 'XGB', 'CAT', 'ET' ì‹œê°í™” ìŠ¤í… ì„¤ì •
    STEP = 10
    # ------------------------------------------------------------

    # 5. ëª¨ë¸ ì„ íƒ
    # ê° íƒ€ê²Ÿì— ì–´ë–¤ ëª¨ë¸ì„ ì‚¬ìš©í• ì§€ ì§€ì •í•©ë‹ˆë‹¤.
    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: 'LGBM', 'XGB', 'CAT', 'ET', 'FTT'
    # ------> ëª¨ë¸ ì—¬ëŸ¬ê°œ ë˜ëŠ” 0ê°œ ì„ íƒ ê°€ëŠ¥ [] or ['LGBM'] or ['LGBM', 'XGB', 'CAT', 'ET', 'FTT']
    MODEL_CONFIG = {
        'Tg': ['LGBM', 'XGB', 'CAT', 'ET', 'FTT'],
        'FFV': ['LGBM', 'XGB', 'CAT', 'ET', 'FTT'],
        'Tc': ['LGBM', 'XGB', 'CAT', 'ET', 'FTT'],
        'Density': ['LGBM', 'XGB', 'CAT', 'ET', 'FTT'],
        'Rg': ['LGBM', 'XGB', 'CAT', 'ET', 'FTT'],
    }
    # ------------------------------------------------------------

    # 6. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    # ê° ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì´ê³³ì—ì„œ ì¡°ì •í•˜ì„¸ìš”.
    HPARAMS = {
        'LGBM': {
            'objective': 'mae',
            'metric': 'mae',
            'n_estimators': 2000,
            'learning_rate': 0.01,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 1,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'num_leaves': 31,
            'verbose': -1,
            'n_jobs': -1,
            'seed': RANDOM_STATE,
        },
        'XGB': {
            'objective': 'reg:squarederror',
            'eval_metric': 'mae',
            'n_estimators': 2000,
            'learning_rate': 0.01,
            'max_depth': 6,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'gamma': 0.1,
            'random_state': RANDOM_STATE,
            'n_jobs': -1,
            'tree_method': 'hist',  # GPU ì‚¬ìš© ì‹œ 'gpu_hist'
        },
        'CAT': {
            'loss_function': 'MAE',
            'eval_metric': 'MAE',
            'iterations': 2000,
            'learning_rate': 0.05,
            'depth': 6,
            'random_seed': RANDOM_STATE,
            'verbose': 200,
            'allow_writing_files': False,
        },
        'ET': {  # Extra Trees
            'n_estimators': 500,
            'criterion': 'squared_error',  # MAEëŠ” ì§€ì›ë˜ì§€ ì•ŠìŒ
            'max_depth': None,
            'min_samples_split': 2,
            'min_samples_leaf': 1,
            'n_jobs': -1,
            'random_state': RANDOM_STATE,
        },
        'FTT': {
            # 1. ëª¨ë¸ ì•„í‚¤í…ì²˜ íŒŒë¼ë¯¸í„°
            'embedding_dim': 64,
            'num_heads': 4,
            'num_layers': 3,
            'ff_hidden_dim': 128,
            'dropout': 0.1,

            # 2. ëª¨ë¸ í•™ìŠµ íŒŒë¼ë¯¸í„°
            'epochs': 100,
            'batch_size': 64,
            'learning_rate': 1e-3,
            'weight_decay': 1e-5,
            'early_stopping_patience': 15,
        },
    }

    # ------------------------------------------------------------
    #  1. ì‚¬ìš©ì ì„¤ì •
    # ------------------------------------------------------------
    # Feature Extraction Flags
    # DO_RDKit = True  # RDKit íŠ¹ì„±ì¶”ì¶œ      ---â”¯--> ë‘˜ ì¤‘ í•˜ë‚˜ ë˜ëŠ” ë‘˜ ë‹¤ ì‚¬ìš©
    # DO_POLYBERT = False  # PolyBERT ì„ë² ë”©    ---â”›
    # DO_FINGERPRINT = False  # í•‘ê±°í”„ë¦°íŠ¸(MFP, MACCS) íŠ¹ì„± ì¶”ê°€
    # RDKit_FILTER_MODE = 'useless'
    #    'required' â†’ required_descriptors âˆª filters_required[label] ì‚¬ìš© (ê³µí†µ + ë¼ë²¨ë³„ í•„ìš”ë§Œ í¬í•¨)
    #    'useless'  â†’ useless_cols âˆª filters_useless[label] ë“œë¡­ (ê³µí†µ + ë¼ë²¨ë³„ ë¶ˆí•„ìš”ë§Œ ì œê±°)
    # ------------------------------------------------------------

    # ------------------------------------------------------------
    # Augmentation Flags 1
    # DO_AUGMENT_SMILES = False  # SMILES Augmentation ì‹¤í–‰ ì—¬ë¶€
    # AUG_NUM = 1  # SMILES ë‹¹ Augmentation ìˆ˜
    # ------------------------------------------------------------

    # ------------------------------------------------------------

    # 1) í•„ìš”í•œ íŠ¹ì„±ë§Œ ì¶”ì¶œ

    # ê³µí†µ
    required_descriptors = {
        'graph_diameter', 'avg_shortest_path', 'num_cycles',
        'MolWt', 'MolLogP', 'TPSA', 'RotatableBonds', 'NumAtoms'
    }
    # ë¼ë²¨ë³„
    filters_required = {
        'Tg': list(set([
            'BalabanJ', 'BertzCT', 'Chi1', 'Chi3n', 'Chi4n', 'EState_VSA4', 'EState_VSA8',
            'FpDensityMorgan3', 'HallKierAlpha', 'Kappa3', 'MaxAbsEStateIndex', 'MolLogP',
            'NumAmideBonds', 'NumHeteroatoms', 'NumHeterocycles', 'NumRotatableBonds',
            'PEOE_VSA14', 'Phi', 'RingCount', 'SMR_VSA1', 'SPS', 'SlogP_VSA1', 'SlogP_VSA5',
            'SlogP_VSA8', 'TPSA', 'VSA_EState1', 'VSA_EState4', 'VSA_EState6', 'VSA_EState7',
            'VSA_EState8', 'fr_C_O_noCOO', 'fr_NH1', 'fr_benzene', 'fr_bicyclic', 'fr_ether',
            'fr_unbrch_alkane'
        ]).union(required_descriptors)),

        'FFV': list(set([
            'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v',
            'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'EState_VSA10', 'EState_VSA5',
            'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'ExactMolWt', 'FpDensityMorgan1',
            'FpDensityMorgan2', 'FpDensityMorgan3', 'FractionCSP3', 'HallKierAlpha',
            'HeavyAtomMolWt', 'Kappa1', 'Kappa2', 'Kappa3', 'MaxAbsEStateIndex',
            'MaxEStateIndex', 'MinEStateIndex', 'MolLogP', 'MolMR', 'MolWt', 'NHOHCount',
            'NOCount', 'NumAromaticHeterocycles', 'NumHAcceptors', 'NumHDonors',
            'NumHeterocycles', 'NumRotatableBonds', 'PEOE_VSA14', 'RingCount', 'SMR_VSA1',
            'SMR_VSA10', 'SMR_VSA3', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA9', 'SPS',
            'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2',
            'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7',
            'SlogP_VSA8', 'TPSA', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2',
            'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7',
            'VSA_EState8', 'VSA_EState9', 'fr_Ar_N', 'fr_C_O', 'fr_NH0', 'fr_NH1',
            'fr_aniline', 'fr_ether', 'fr_halogen', 'fr_thiophene'
        ]).union(required_descriptors)),

        'Tc': list(set([
            'BalabanJ', 'BertzCT', 'Chi0', 'EState_VSA5', 'ExactMolWt', 'FpDensityMorgan1',
            'FpDensityMorgan2', 'FpDensityMorgan3', 'HeavyAtomMolWt', 'MinEStateIndex',
            'MolWt', 'NumAtomStereoCenters', 'NumRotatableBonds', 'NumValenceElectrons',
            'SMR_VSA10', 'SMR_VSA7', 'SPS', 'SlogP_VSA6', 'SlogP_VSA8', 'VSA_EState1',
            'VSA_EState7', 'fr_NH1', 'fr_ester', 'fr_halogen'
        ]).union(required_descriptors)),

        'Density': list(set([
            'BalabanJ', 'Chi3n', 'Chi3v', 'Chi4n', 'EState_VSA1', 'ExactMolWt',
            'FractionCSP3', 'HallKierAlpha', 'Kappa2', 'MinEStateIndex', 'MolMR', 'MolWt',
            'NumAliphaticCarbocycles', 'NumHAcceptors', 'NumHeteroatoms',
            'NumRotatableBonds', 'SMR_VSA10', 'SMR_VSA5', 'SlogP_VSA12', 'SlogP_VSA5',
            'TPSA', 'VSA_EState10', 'VSA_EState7', 'VSA_EState8'
        ]).union(required_descriptors)),

        'Rg': list(set([
            'AvgIpc', 'Chi0n', 'Chi1v', 'Chi2n', 'Chi3v', 'ExactMolWt', 'FpDensityMorgan1',
            'FpDensityMorgan2', 'FpDensityMorgan3', 'HallKierAlpha', 'HeavyAtomMolWt',
            'Kappa3', 'MaxAbsEStateIndex', 'MolWt', 'NOCount', 'NumRotatableBonds',
            'NumUnspecifiedAtomStereoCenters', 'NumValenceElectrons', 'PEOE_VSA14',
            'PEOE_VSA6', 'SMR_VSA1', 'SMR_VSA5', 'SPS', 'SlogP_VSA1', 'SlogP_VSA2',
            'SlogP_VSA7', 'SlogP_VSA8', 'VSA_EState1', 'VSA_EState8', 'fr_alkyl_halide',
            'fr_halogen'
        ]).union(required_descriptors))
    }

    # ë“œë¡­í•  ë¶ˆí•„ìš” ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    useless_cols = [

        'MaxPartialCharge',
        # Nan data
        'BCUT2D_MWHI',
        'BCUT2D_MWLOW',
        'BCUT2D_CHGHI',
        'BCUT2D_CHGLO',
        'BCUT2D_LOGPHI',
        'BCUT2D_LOGPLOW',
        'BCUT2D_MRHI',
        'BCUT2D_MRLOW',

        # Constant data
        'NumRadicalElectrons',
        'SMR_VSA8',
        'SlogP_VSA9',
        'fr_barbitur',
        'fr_benzodiazepine',
        'fr_dihydropyridine',
        'fr_epoxide',
        'fr_isothiocyan',
        'fr_lactam',
        'fr_nitroso',
        'fr_prisulfonamd',
        'fr_thiocyan',

        # High correlated data >0.95
        'MaxEStateIndex',
        'HeavyAtomMolWt',
        'ExactMolWt',
        'NumValenceElectrons',
        'Chi0',
        'Chi0n',
        'Chi0v',
        'Chi1',
        'Chi1n',
        'Chi1v',
        'Chi2n',
        'Kappa1',
        'LabuteASA',
        'HeavyAtomCount',
        'MolMR',
        'Chi3n',
        'BertzCT',
        'Chi2v',
        'Chi4n',
        'HallKierAlpha',
        'Chi3v',
        'Chi4v',
        'MinAbsPartialCharge',
        'MinPartialCharge',
        'MaxAbsPartialCharge',
        'FpDensityMorgan2',
        'FpDensityMorgan3',
        'Phi',
        'Kappa3',
        'fr_nitrile',
        'SlogP_VSA6',
        'NumAromaticCarbocycles',
        'NumAromaticRings',
        'fr_benzene',
        'VSA_EState6',
        'NOCount',
        'fr_C_O',
        'fr_C_O_noCOO',
        'NumHDonors',
        'fr_amide',
        'fr_Nhpyrrole',
        'fr_phenol',
        'fr_phenol_noOrthoHbond',
        'fr_COO2',
        'fr_halogen',
        'fr_diazo',
        'fr_nitro_arom',
        'fr_phos_ester'
    ]

    filters_useless = {
        'Tg': [],
        'FFV': [],
        'Tc': [],
        'Density': [],
        'Rg': [],
    }

    # 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
    import os
    import re
    from pathlib import Path
    import joblib
    import math

    import numpy as np
    import pandas as pd

    from sklearn.preprocessing import StandardScaler, normalize
    from sklearn.feature_selection import VarianceThreshold
    from sklearn.mixture import GaussianMixture

    import torch
    from sentence_transformers.SentenceTransformer import SentenceTransformer

    from rdkit import Chem
    from rdkit.Chem import Descriptors, rdMolDescriptors, MACCSkeys
    from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator

    import networkx as nx
    from tqdm import tqdm

    # 3. ê²½ë¡œ ì„¤ì •
    INPUT_DIR = Path("Dataset/neurips-open-polymer-prediction-2025")
    TRAIN_CSV = INPUT_DIR / "train.csv"
    TEST_CSV = INPUT_DIR / "test.csv"
    WORK_DIR = OUTPUT_DIR / Path("features")
    WORK_DIR.mkdir(exist_ok=True, parents=True)

    # PolyBERT ëª¨ë¸ ë¡œë“œ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    MODEL_PATH = Path("Dataset") / "polyBERT" / "polyBERT-local"
    polybert = SentenceTransformer(str(MODEL_PATH.resolve()), device=device)

    # 4. ìœ í‹¸ í•¨ìˆ˜
    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']

    def get_canonical_smiles(smiles):
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol:
                return Chem.MolToSmiles(mol, canonical=True)
        except:
            pass
        return None

    def clean_and_validate_smiles(smiles):
        bad_patterns = [
            '[R]', '[R1]', '[R2]', '[R3]', '[R4]', '[R5]',
            "[R']", '[R"]', 'R1', 'R2', 'R3', 'R4', 'R5',
            '([R])', '([R1])', '([R2])'
        ]
        if not isinstance(smiles, str) or not smiles:
            return None
        for p in bad_patterns:
            if p in smiles:
                return None
        if '][' in smiles and any(x in smiles for x in ['[R', 'R]']):
            return None
        return get_canonical_smiles(smiles)

    def separate_subtables(train_df):
        return {
            label: train_df.loc[train_df[label].notna(), ['SMILES', label]].reset_index(drop=True)
            for label in TARGETS
        }

    def augment_smiles_dataset(smiles_list, labels, num_augments=AUG_NUM):
        aug_smi, aug_lbl = [], []
        for smi, lab in zip(smiles_list, labels):
            mol = Chem.MolFromSmiles(smi)
            if mol is None:
                continue
            aug_smi.append(smi);
            aug_lbl.append(lab)
            for _ in range(num_augments):
                aug_smi.append(Chem.MolToSmiles(mol, doRandom=True))
                aug_lbl.append(lab)
        return aug_smi, np.array(aug_lbl)

    def smiles_to_combined_fingerprints_with_descriptors(smiles_list, radius=2, n_bits=128):
        gen = GetMorganGenerator(radius=radius, fpSize=n_bits)
        fps, descs, valid, invalid_idx = [], [], [], []
        for i, smi in enumerate(smiles_list):
            mol = Chem.MolFromSmiles(smi)
            if mol:
                mfp = np.array(gen.GetFingerprint(mol))
                maccs = np.array(MACCSkeys.GenMACCSKeys(mol))
                fps.append(np.concatenate([mfp, maccs]))
                dv = {}
                for name, fn in Descriptors.descList:
                    try:
                        dv[name] = fn(mol)
                    except:
                        dv[name] = np.nan
                dv.update({
                    'MolWt': Chem.Descriptors.MolWt(mol),
                    'LogP': Chem.Descriptors.MolLogP(mol),
                    'TPSA': rdMolDescriptors.CalcTPSA(mol),
                    'RotatableBonds': rdMolDescriptors.CalcNumRotatableBonds(mol),
                    'NumAtoms': mol.GetNumAtoms()
                })
                adj = Chem.rdmolops.GetAdjacencyMatrix(mol)
                G = nx.from_numpy_array(adj)
                if nx.is_connected(G):
                    dv['graph_diameter'] = nx.diameter(G)
                    dv['avg_shortest_path'] = nx.average_shortest_path_length(G)
                else:
                    dv['graph_diameter'] = dv['avg_shortest_path'] = 0
                dv['num_cycles'] = len(list(nx.cycle_basis(G)))
                descs.append(dv);
                valid.append(smi)
            else:
                fps.append(np.zeros(n_bits + 167))
                descs.append({});
                invalid_idx.append(i);
                valid.append(None)
        return np.array(fps), descs, valid, invalid_idx

    def extract_polybert(smiles_list):
        emb = polybert.encode(
            smiles_list,
            convert_to_numpy=True,
            show_progress_bar=True
        )
        return normalize(emb, axis=1)

    print("Library loading complete")

    # 5. ì™¸ë¶€ ë°ì´í„°ì…‹ ì„¤ì •

    # 1. ê¸°ë³¸ ë°ì´í„° ë¡œë“œ
    BASE_PATH = 'Dataset/neurips-open-polymer-prediction-2025/'
    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']

    print("ğŸ“‚ Loading base train/test data...")
    train = pd.read_csv(TRAIN_CSV)
    test_df = pd.read_csv(TEST_CSV)

    # Clean SMILES í•¨ìˆ˜ê°€ ë°˜ë“œì‹œ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨
    train['SMILES'] = train['SMILES'].apply(clean_and_validate_smiles)
    test_df['SMILES'] = test_df['SMILES'].apply(clean_and_validate_smiles)

    train = train[train['SMILES'].notnull()].reset_index(drop=True)

    print(f"âœ… Base training samples: {len(train)}")
    print(f"âœ… Base test samples: {len(test_df)}")

    print("\nğŸ“‚ Loading external datasets...")

    # 2. ì™¸ë¶€ ë°ì´í„°ì…‹ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œ
    external_datasets = []

    def safe_load_dataset(path, target, processor_func, description):
        try:
            if path.endswith('.xlsx'):
                data = pd.read_excel(path)
            else:
                data = pd.read_csv(path)
            data = processor_func(data)
            external_datasets.append((target, data))
            print(f"   âœ… {description}: {len(data)} samples")
            return True
        except Exception as e:
            print(f"   âš ï¸ {description} failed: {str(e)[:100]}")
            return False

    # 3. ì™¸ë¶€ ë°ì´í„° ë¡œë“œ
    safe_load_dataset('Dataset/tc_SMILES/tc_SMILES.csv', 'Tc',
                      lambda df: df.rename(columns={'TC_mean': 'Tc'}),
                      'Tc data')

    safe_load_dataset('Dataset/tg_SMILES_PID_Polymer Class/tgSS_enriched_cleaned.csv', 'Tg',
                      lambda df: df[['SMILES', 'Tg']] if 'Tg' in df.columns else df,
                      'TgSS enriched data')

    safe_load_dataset('Dataset/smiles-extra-data/JCIM_sup_bigsmiles.csv', 'Tg',
                      lambda df: df[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'}),
                      'JCIM Tg data')

    safe_load_dataset('Dataset/smiles-extra-data/data_dnst1.xlsx', 'Density',
                      lambda df: df.rename(columns={'density(g/cm3)': 'Density'})[['SMILES', 'Density']]
                      .query('SMILES.notnull() and Density.notnull() and Density != "nylon"')
                      .assign(Density=lambda x: x['Density'].astype(float) - 0.118),
                      'Density data')

    # 4. ì™¸ë¶€ ë°ì´í„° ë³‘í•©
    def add_extra_data_clean(df_train, df_extra, target):
        # ì™¸ë¶€ ë°ì´í„°ì…‹ì—ì„œ SMILES ì •ì œ í›„, target ê°’ì´ ìˆëŠ” ë°ì´í„°ë§Œ df_trainì— ë³‘í•©
        df_extra['SMILES'] = df_extra['SMILES'].apply(clean_and_validate_smiles)
        df_extra = df_extra[df_extra['SMILES'].notnull()]
        df_extra = df_extra.dropna(subset=[target])
        if len(df_extra) == 0:
            return df_train

        # SMILESë³„ í‰ê· ê°’ (ì¤‘ë³µ SMILES ì²˜ë¦¬)
        df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()

        # trainì— ì—†ëŠ” ìƒˆë¡œìš´ SMILESë§Œ ì¶”ê°€
        unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])
        extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()

        if len(extra_to_add) > 0:
            # ëª¨ë“  íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ë§ì¶°ì„œ ì¶”ê°€
            for col in TARGETS:
                if col not in extra_to_add.columns:
                    extra_to_add[col] = np.nan
            extra_to_add = extra_to_add[['SMILES'] + TARGETS]
            df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)

        return df_train

    # 5. ë³‘í•© ì‹¤í–‰
    train_extended = train[['SMILES'] + TARGETS].copy()
    for target, dataset in external_datasets:
        train_extended = add_extra_data_clean(train_extended, dataset, target)

    # 6. í•© í›„ ìµœì¢… clean ì²˜ë¦¬
    train_extended['SMILES'] = train_extended['SMILES'].apply(clean_and_validate_smiles)
    train_extended = train_extended[train_extended['SMILES'].notnull()].reset_index(drop=True)
    train_df = train_extended

    print(f"\nâœ… Final extended training samples: {len(train_extended)}")

    def main(target_labels=None):
        """
        target_labels: list of labels to process (e.g. ['Tg','Density'])
                       None ì´ë©´ ëª¨ë“  TARGETS ì²˜ë¦¬
        """
        labels = TARGETS if target_labels is None else target_labels

        # subtables ì¤€ë¹„
        subtables = {
            lbl: train_extended.loc[train_extended[lbl].notna(), ['SMILES', lbl]]
            .reset_index(drop=True)
            for lbl in labels
        }

        for label in labels:
            # â–¶ drop ë¦¬ìŠ¤íŠ¸ ì •ì˜ (ë£¨í”„ ì•ˆì—ì„œ)
            drop_useless = list(set(useless_cols) | set(filters_useless[label]))
            drop_required = [
                'BCUT2D_MWLOW', 'BCUT2D_MWHI', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO',
                'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRLOW', 'BCUT2D_MRHI',
                'MinAbsPartialCharge', 'MaxPartialCharge',
                'MinPartialCharge', 'MaxAbsPartialCharge', 'SMILES'
            ]

            df_lbl = subtables[label]
            smiles = df_lbl['SMILES'].tolist()
            y_tr = df_lbl[label].values

            # 1) SMILES ì¦ê°•
            if DO_AUGMENT_SMILES:
                print(f"\n[{label}] 1) SMILES ì¦ê°•")
                aug_smiles, aug_labels = [], []
                for smi, lab in tqdm(zip(smiles, y_tr),
                                     total=len(smiles),
                                     desc=f"{label} SMILES ì¦ê°•"):
                    mol = Chem.MolFromSmiles(smi)
                    if mol is None:
                        continue
                    aug_smiles.append(smi)
                    aug_labels.append(lab)
                    for _ in range(AUG_NUM):
                        aug_smiles.append(Chem.MolToSmiles(mol, doRandom=True))
                        aug_labels.append(lab)
                smiles, y_tr = aug_smiles, np.array(aug_labels)
            else:
                print(f"[{label}] 1) SMILES ì¦ê°• skipped (DO_AUGMENT_SMILES=False)")

            # 2) RDKit descriptor ë° í•‘ê±°í”„ë¦°íŠ¸ ì¶”ì¶œ
            if DO_RDKit:
                print(f"[{label}] 2) RDKit descriptor ì¶”ì¶œ & í•„í„°ë§")
                fps, descs = [], []
                for smi in tqdm(smiles,
                                total=len(smiles),
                                desc=f"{label} RDKit ì¶”ì¶œ"):
                    mol = Chem.MolFromSmiles(smi)
                    if mol:
                        if DO_FINGERPRINT:
                            mfp = np.array(GetMorganGenerator(radius=2, fpSize=128).GetFingerprint(mol))
                            maccs = np.array(MACCSkeys.GenMACCSKeys(mol))
                            fps.append(np.concatenate([mfp, maccs]))
                        dv = {}
                        for name, fn in Descriptors.descList:
                            try:
                                dv[name] = fn(mol)
                            except:
                                dv[name] = np.nan
                        dv.update({
                            'MolWt': Chem.Descriptors.MolWt(mol),
                            'LogP': Chem.Descriptors.MolLogP(mol),
                            'TPSA': rdMolDescriptors.CalcTPSA(mol),
                            'RotatableBonds': rdMolDescriptors.CalcNumRotatableBonds(mol),
                            'NumAtoms': mol.GetNumAtoms()
                        })
                        adj = Chem.rdmolops.GetAdjacencyMatrix(mol)
                        G = nx.from_numpy_array(adj)
                        if nx.is_connected(G):
                            dv['graph_diameter'] = nx.diameter(G)
                            dv['avg_shortest_path'] = nx.average_shortest_path_length(G)
                        else:
                            dv['graph_diameter'] = dv['avg_shortest_path'] = 0
                        dv['num_cycles'] = len(list(nx.cycle_basis(G)))
                        descs.append(dv)
                    else:
                        if DO_FINGERPRINT:
                            fps.append(np.zeros(128 + 167))
                        descs.append({})

                # RDKit ì„œìˆ ì DataFrame ìƒì„± ë° í•„í„°ë§
                df_desc = pd.DataFrame(descs).fillna(0)
                if RDKit_FILTER_MODE == 'useless':
                    # 'useless' ëª¨ë“œ: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ë“¤ì„ ì •ì˜í•˜ê³  ì œê±°í•©ë‹ˆë‹¤.
                    cols_to_drop = list(set(useless_cols) | set(filters_useless.get(label, set())))
                    df_desc.drop(columns=cols_to_drop, errors='ignore', inplace=True)
                    print(f"[{label}] 'useless' ëª¨ë“œ ì ìš©. {len(cols_to_drop)}ê°œ ê·œì¹™ìœ¼ë¡œ íŠ¹ì„± ì œê±°.")

                    # í•„í„°ë§ í›„, inf ê°’ì„ NaNìœ¼ë¡œ ë³€ê²½í•˜ê³ , NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°
                    df_desc.replace([np.inf, -np.inf], np.nan, inplace=True)

                elif RDKit_FILTER_MODE == 'required':
                    # 'required' ëª¨ë“œ: í•„ìš”í•œ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒí•©ë‹ˆë‹¤.
                    # filters_required[label]ì€ ì´ë¯¸ ê³µí†µ+ë¼ë²¨ë³„ ë¦¬ìŠ¤íŠ¸ì˜ í•©ì§‘í•©ìœ¼ë¡œ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
                    required_cols = filters_required.get(label, [])
                    df_desc = df_desc.filter(items=required_cols, axis=1)
                    print(f"[{label}] 'required' ëª¨ë“œ ì ìš©. {len(required_cols)}ê°œ íŠ¹ì„± ì„ íƒ.")

                # í•‘ê±°í”„ë¦°íŠ¸ DataFrame ìƒì„±
                if DO_FINGERPRINT:
                    fp_cols = [f'MFP_{i}' for i in range(128)] + [f'MACCS_{i}' for i in range(167)]
                    df_fps = pd.DataFrame(fps, columns=fp_cols)
                else:
                    df_fps = pd.DataFrame(index=range(len(smiles)))
            else:
                print(f"[{label}] 2) RDKit descriptor ì¶”ì¶œ skipped (DO_RDKit=False)")
                # ë¹ˆ DataFrame í• ë‹¹
                df_desc = pd.DataFrame(index=range(len(smiles)))
                df_fps = pd.DataFrame(index=range(len(smiles)))

            # 3) PolyBERT embedding
            if DO_POLYBERT:
                print(f"[{label}] 3) PolyBERT embedding")
                emb_list = []
                batch_size = 64
                for i in tqdm(range(0, len(smiles), batch_size),
                              total=math.ceil(len(smiles) / batch_size),
                              desc=f"{label} PolyBERT embedding"):
                    batch = smiles[i: i + batch_size]
                    emb_batch = polybert.encode(batch, convert_to_numpy=True, show_progress_bar=False)
                    emb_list.append(emb_batch)
                emb = normalize(np.vstack(emb_list), axis=1)
            else:
                print(f"[{label}] 3) PolyBERT embedding skipped (DO_POLYBERT=False)")
                emb = np.empty((len(smiles), 0), dtype=np.float64)

            # 4) Feature ê²°í•© (DataFrame ì‚¬ìš©)
            print(f"[{label}] 4) Feature ê²°í•©")
            # RDKit ë˜ëŠ” PolyBERT ì¤‘ í•˜ë‚˜ë¼ë„ ì¼œì ¸ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
            if not (DO_RDKit or DO_POLYBERT):
                raise RuntimeError(
                    f"[{label}] No features to combine: both DO_RDKit and DO_POLYBERT are False."
                )

            # PolyBERT ì„ë² ë”©ì„ DataFrameìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
            df_emb = pd.DataFrame(emb, columns=[f'PolyBERT_{i}' for i in range(emb.shape[1])])

            # ëª¨ë“  íŠ¹ì„± DataFrameì„ ìˆ˜í‰ìœ¼ë¡œ ê²°í•©í•©ë‹ˆë‹¤.
            for _ in tqdm(range(1), desc=f"{label} Feature ê²°í•©"):
                X_tr_df = pd.concat([df_desc, df_fps, df_emb], axis=1)

                # ë¬´í•œëŒ€(inf) ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜í•˜ê³  0ìœ¼ë¡œ ì±„ìš°ê¸°
                X_tr_df.replace([np.inf, -np.inf], np.nan, inplace=True)
                X_tr_df.fillna(0, inplace=True)

                print(f"[{label}] ê²°í•© í›„ íŠ¹ì„± ê°œìˆ˜: {X_tr_df.shape[1]}")

            # ìµœì¢… ë°ì´í„°ì™€ ì»¬ëŸ¼ ì´ë¦„ ì €ì¥
            X_tr_final_df = X_tr_df
            final_feature_names = X_tr_final_df.columns.tolist()
            joblib.dump(final_feature_names, WORK_DIR / f"all_feature_names_{label}.pkl")

            X_tr_final_np = X_tr_final_df.values

            # ë°ì´í„° ì €ì¥
            np.save(WORK_DIR / f"X_train_{label}.npy", X_tr_final_np)
            np.save(WORK_DIR / f"y_train_{label}.npy", y_tr)

            print(f"âœ… [{label}] ìµœì¢… ë°ì´í„°(shape:{X_tr_final_np.shape})ì™€ "
                  f"{len(final_feature_names)}ê°œì˜ íŠ¹ì„± ì´ë¦„ ì €ì¥ ì™„ë£Œ.")

        train_extended.to_csv(WORK_DIR / "train_merged.csv", index=False)
        print("âœ… ëª¨ë“  ë¼ë²¨ ì²˜ë¦¬ ì™„ë£Œ.")

    def main_test(target_labels=None):
        """
        í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì €ì¥ (raw featuresë§Œ ìƒì„±)
        - ëª¨ë¸ íŒŒì´í”„ë¼ì¸ì´ ìŠ¤ìŠ¤ë¡œ ë™ì¼ ì „ì²˜ë¦¬ ê·œì¹™ì„ ì ìš©í•˜ë¯€ë¡œ,
          main_test ë‹¨ê³„ì—ì„  ì˜¤ì§ feature matrix ìƒì„±/ì €ì¥ë§Œ ìˆ˜í–‰
        """
        labels = TARGETS if target_labels is None else target_labels
        smis_te, n_samples = test_df['SMILES'].tolist(), len(test_df)

        for label in labels:
            print(f"\n[{label}] Test Data Preprocessing for {label}")

            # â–¶ drop ë¦¬ìŠ¤íŠ¸ ì •ì˜
            drop_useless = list(set(useless_cols) | set(filters_useless[label]))
            drop_required = [
                'BCUT2D_MWLOW', 'BCUT2D_MWHI', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO',
                'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRLOW', 'BCUT2D_MRHI',
                'MinAbsPartialCharge', 'MaxPartialCharge',
                'MinPartialCharge', 'MaxAbsPartialCharge', 'SMILES'
            ]

            # 1) SMILES ì¦ê°• (í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” ì ìš©í•˜ì§€ ì•ŠìŒ)
            print(f"[{label}] 1) SMILES ì¦ê°• skipped (Test data)")

            # 2) RDKit descriptor ë° fingerprint ì¶”ì¶œ
            if DO_RDKit:
                print(f"[{label}] 2) Test RDKit descriptor & fingerprint ì¶”ì¶œ")
                fps_te, descs_te = [], []
                for smi in tqdm(smis_te, total=n_samples, desc=f"{label} Test RDKit"):
                    mol = None
                    if isinstance(smi, str):
                        mol = Chem.MolFromSmiles(smi)

                    if mol:
                        if DO_FINGERPRINT:
                            mfp = np.array(GetMorganGenerator(radius=2, fpSize=128).GetFingerprint(mol))
                            maccs = np.array(MACCSkeys.GenMACCSKeys(mol))
                            fps_te.append(np.concatenate([mfp, maccs]))
                        dv = {}
                        for name, fn in Descriptors.descList:
                            try:
                                dv[name] = fn(mol)
                            except:
                                dv[name] = np.nan
                        dv.update({
                            'MolWt': Chem.Descriptors.MolWt(mol), 'LogP': Chem.Descriptors.MolLogP(mol),
                            'TPSA': rdMolDescriptors.CalcTPSA(mol),
                            'RotatableBonds': rdMolDescriptors.CalcNumRotatableBonds(mol),
                            'NumAtoms': mol.GetNumAtoms()
                        })
                        adj = Chem.rdmolops.GetAdjacencyMatrix(mol);
                        G = nx.from_numpy_array(adj)
                        if nx.is_connected(G):
                            dv['graph_diameter'] = nx.diameter(G);
                            dv['avg_shortest_path'] = nx.average_shortest_path_length(G)
                        else:
                            dv['graph_diameter'] = dv['avg_shortest_path'] = 0
                        dv['num_cycles'] = len(list(nx.cycle_basis(G)))
                        descs_te.append(dv)
                    else:
                        if DO_FINGERPRINT: fps_te.append(np.zeros(128 + 167))
                        descs_te.append({})
                df_desc_te = pd.DataFrame(descs_te).fillna(0)

                if RDKit_FILTER_MODE == 'useless':
                    # ê³µí†µ useless_cols + ë¼ë²¨ë³„ filters_useless[label] ì„ drop
                    cols_to_drop = list(set(useless_cols) | set(filters_useless[label]))
                    df_desc_te.drop(columns=cols_to_drop, errors='ignore', inplace=True)
                elif RDKit_FILTER_MODE == 'required':
                    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë‚¨ê¹€
                    req = filters_required[label]  # ì´ë¯¸ required_descriptors âˆª ë¼ë²¨ë³„ í•„ìš” ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë¼ ìˆìŒ
                    df_desc_te = df_desc_te[req]

                if DO_FINGERPRINT:
                    fp_cols = [f'MFP_{i}' for i in range(128)] + [f'MACCS_{i}' for i in range(167)]
                    df_fps_te = pd.DataFrame(fps_te, columns=fp_cols)
                else:
                    df_fps_te = pd.DataFrame(index=range(n_samples))
            else:
                print(f"[{label}] 2) Test RDKit descriptor ì¶”ì¶œ skipped (DO_RDKit=False)")
                df_desc_te = pd.DataFrame(index=range(n_samples))
                df_fps_te = pd.DataFrame(index=range(n_samples))

            # 3) PolyBERT embedding
            if DO_POLYBERT:
                print(f"[{label}] 3) Test PolyBERT embedding")
                emb_te_list = []
                batch_size = 64
                for i in tqdm(range(0, n_samples, batch_size),
                              total=math.ceil(n_samples / batch_size),
                              desc=f"{label} Test PolyBERT embedding"):
                    batch = smis_te[i: i + batch_size]

                    safe_batch = [s if isinstance(s, str) else '' for s in batch]
                    emb_batch = polybert.encode(safe_batch, convert_to_numpy=True, show_progress_bar=False)
                    emb_te_list.append(emb_batch)

                emb_te = normalize(np.vstack(emb_te_list), axis=1)
            else:
                print(f"[{label}] 3) PolyBERT embedding skipped (DO_POLYBERT=False)")
                emb_te = np.empty((n_samples, 0), dtype=np.float64)

            # 4) Feature ê²°í•©
            print(f"[{label}] 4) Feature ê²°í•©")
            if not (DO_RDKit or DO_POLYBERT):
                raise RuntimeError(f"[{label}] No features to combine: both DO_RDKit and DO_POLYBERT are False.")
            df_emb_te = pd.DataFrame(emb_te, columns=[f'PolyBERT_{i}' for i in range(emb_te.shape[1])])
            X_te_df_initial = pd.concat([df_desc_te, df_fps_te, df_emb_te], axis=1)

            # ë¬´í•œëŒ€(inf) ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
            X_te_df_initial.replace([np.inf, -np.inf], np.nan, inplace=True)

            print(f"[{label}] ê²°í•© í›„ íŠ¹ì„± ê°œìˆ˜: {X_te_df_initial.shape[1]}")

            # í›ˆë ¨ì‹œ ì €ì¥ëœ ì „ì²´ feature ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°, ìˆœì„œ ë§ì¶”ê¸°ë§Œ ìˆ˜í–‰
            all_feature_names = joblib.load(INPUT_DIR / f"all_feature_names_{label}.pkl")
            X_te_df_initial = X_te_df_initial.reindex(columns=all_feature_names, fill_value=0)
            print(f"[{label}] Test ë°ì´í„° íŠ¹ì„±ì„ í›ˆë ¨ ìŠ¤í‚¤ë§ˆì— ë§ì¶° ì¬ì •ë ¬ ì™„ë£Œ. Shape: {X_te_df_initial.shape}")

            # 6) npy ì €ì¥
            X_te_processed = X_te_df_initial.values
            np.save(WORK_DIR / f"X_test_{label}.npy", X_te_processed)
            print(f"âœ… [{label}] Final test data saved. Shape: {X_te_processed.shape}")

        test_df.to_csv(WORK_DIR / "test_cleaned.csv", index=False)
        print("âœ… All test features processed.")

    # ------------------------------------------------------------
    # âš¡ï¸ (ì„ íƒ) ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤í–‰
    # ------------------------------------------------------------
    # RUN_PREPROCESSING í”Œë˜ê·¸ ê°’ì— ë”°ë¼ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

    if RUN_PREPROCESSING:
        print("RUN_PREPROCESSING=True. ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

        # ì „ì²˜ë¦¬, ì „ì²´ TARGETS ì²˜ë¦¬: main() ë˜ëŠ” ì¼ë¶€ë§Œ: main(['Tg','Density'])
        main(PREPROCESS_TARGET)

        # ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ, ì´í›„ í›ˆë ¨ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ë°ì´í„° ê²½ë¡œë¥¼
        # ì „ì²˜ë¦¬ ê²°ê³¼ë¬¼ì´ ìˆëŠ” í´ë”ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
        INPUT_DIR = OUTPUT_DIR / Path("features")
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ. ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ INPUT_DIRì´ '{INPUT_DIR}'(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        INPUT_DIR = PRE_INPUT_DIR
        print(f"Skipped Preprocessing (RUN_PREPROCESSING=False)")

    # ------------------------------------------------------------
    # ğŸ§  FTTransformer ëª¨ë¸ í´ë˜ìŠ¤
    # ğŸ“¦ PyTorch ë°ì´í„°ì…‹ ë° í—¬í¼ í•¨ìˆ˜
    # ------------------------------------------------------------
    class FTEmbedding(nn.Module):
        def __init__(self, categories, num_continuous, embedding_dim):
            super().__init__()
            # ë²”ì£¼í˜• íŠ¹ì„±ì´ ìˆì„ ë•Œë§Œ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ìƒì„±
            if categories:
                self.cat_embeddings = nn.ModuleList([
                    nn.Embedding(num_cat, embedding_dim) for num_cat in categories
                ])
            else:
                self.cat_embeddings = None

            self.cont_emb = nn.Linear(num_continuous, embedding_dim) if num_continuous > 0 else None
            self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))

            if self.cat_embeddings:
                for emb in self.cat_embeddings:
                    nn.init.xavier_uniform_(emb.weight)
            if self.cont_emb:
                nn.init.xavier_uniform_(self.cont_emb.weight)

        def forward(self, x_cat, x_cont):
            B = x_cat.size(0) if x_cat.nelement() > 0 else x_cont.size(0)
            tokens_list = []

            # ë²”ì£¼í˜• íŠ¹ì„±ì´ ìˆì„ ë•Œë§Œ ì„ë² ë”© ë° ìŠ¤íƒ ì—°ì‚° ìˆ˜í–‰
            if self.cat_embeddings and x_cat.nelement() > 0:
                cat_tokens = torch.stack([
                    emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeddings)
                ], dim=1)
                tokens_list.append(cat_tokens)

            if x_cont is not None and self.cont_emb and x_cont.nelement() > 0:
                cont_token = self.cont_emb(x_cont).unsqueeze(1)
                tokens_list.append(cont_token)

            # ë¦¬ìŠ¤íŠ¸ ë§¨ ì•ì— CLS í† í° ì¶”ê°€
            cls_tokens = self.cls_token.expand(B, -1, -1)
            tokens_list.insert(0, cls_tokens)

            if not tokens_list:
                # ë§Œì•½ ë²”ì£¼í˜•, ì—°ì†í˜• íŠ¹ì„±ì´ ëª¨ë‘ ì—†ëŠ” ê·¹ë‹¨ì ì¸ ê²½ìš°
                return self.cls_token.expand(B, -1, -1)

            return torch.cat(tokens_list, dim=1)

    class TransformerBlock(nn.Module):
        def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):
            super().__init__()
            self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
            self.norm1 = nn.LayerNorm(embed_dim)
            self.ff = nn.Sequential(
                nn.Linear(embed_dim, ff_hidden_dim), nn.ReLU(),
                nn.Dropout(dropout), nn.Linear(ff_hidden_dim, embed_dim)
            )
            self.norm2 = nn.LayerNorm(embed_dim)
            self.dropout = nn.Dropout(dropout)

        def forward(self, x):
            attn_out, _ = self.attn(x, x, x)
            x = self.norm1(x + self.dropout(attn_out))
            ff_out = self.ff(x)
            x = self.norm2(x + self.dropout(ff_out))
            return x

    class FTTransformer(nn.Module):
        def __init__(self, categories, num_continuous, embedding_dim, num_heads,
                     num_layers, ff_hidden_dim, dropout=0.1, output_dim=1):
            super().__init__()
            self.embedding = FTEmbedding(categories, num_continuous, embedding_dim)
            self.transformer_blocks = nn.Sequential(*[
                TransformerBlock(embedding_dim, num_heads, ff_hidden_dim, dropout)
                for _ in range(num_layers)
            ])
            self.mlp_head = nn.Sequential(
                nn.Linear(embedding_dim, ff_hidden_dim), nn.ReLU(),
                nn.Dropout(dropout), nn.Linear(ff_hidden_dim, output_dim)
            )

        def forward(self, x_cat, x_cont):
            tokens = self.embedding(x_cat, x_cont)
            tokens = self.transformer_blocks(tokens)
            cls_token = tokens[:, 0]
            return self.mlp_head(cls_token).squeeze(-1)

    class TabularDataset(Dataset):
        def __init__(self, X_cat, X_cont, y=None):
            self.X_cat = torch.tensor(X_cat, dtype=torch.long)
            self.X_cont = torch.tensor(X_cont, dtype=torch.float32)
            self.y = torch.tensor(y, dtype=torch.float32) if y is not None else None

            self.length = len(self.X_cat) if self.X_cat.nelement() > 0 else len(self.X_cont)

        def __len__(self):
            return self.length

        def __getitem__(self, idx):
            if self.y is not None:
                return self.X_cat[idx], self.X_cont[idx], self.y[idx]
            return self.X_cat[idx], self.X_cont[idx]

    def get_model(model_name, hparams, categories=None, num_continuous=None):
        if model_name == 'LGBM': return lgb.LGBMRegressor(**hparams)
        if model_name == 'XGB': return xgb.XGBRegressor(**hparams)
        if model_name == 'CAT': return cb.CatBoostRegressor(**hparams)
        if model_name == 'ET': return ExtraTreesRegressor(**hparams)
        if model_name == 'FTT':
            return FTTransformer(
                categories=categories, num_continuous=num_continuous,
                embedding_dim=hparams['embedding_dim'], num_heads=hparams['num_heads'],
                num_layers=hparams['num_layers'], ff_hidden_dim=hparams['ff_hidden_dim'],
                dropout=hparams['dropout']
            ).to(DEVICE)
        raise ValueError(f"Unknown model: {model_name}")

    def get_tree_model_history(model, model_name):
        """LGBM, XGB, CAT ëª¨ë¸ì—ì„œ í›ˆë ¨ ë° ê²€ì¦ ê¸°ë¡ì„ ëª¨ë‘ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            if model_name == 'LGBM':
                results = model.evals_result_
                # í•´ê²°ì±…: Metric ì´ë¦„ì„ í•˜ë“œì½”ë”©í•˜ëŠ” ëŒ€ì‹ , ë™ì ìœ¼ë¡œ ì²« ë²ˆì§¸ í‚¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                #   ì´ë ‡ê²Œ í•˜ë©´ LightGBMì´ 'mae'ë¥¼ ì“°ë“  'l1'ì„ ì“°ë“  ìƒê´€ì—†ì´ ì‘ë™í•©ë‹ˆë‹¤.
                train_metric_key = list(results['train'].keys())[0]
                valid_metric_key = list(results['valid'].keys())[0]

                return {
                    'train_metric': results['train'][train_metric_key],
                    'val_metric': results['valid'][valid_metric_key],
                    'metric_name': 'MAE'  # ê·¸ë˜í”„ì— í‘œì‹œë  ì´ë¦„ì€ 'MAE'ë¡œ ê³ ì •
                }
            if model_name == 'XGB':
                results = model.evals_result()
                return {
                    'train_metric': results['validation_0']['mae'],
                    'val_metric': results['validation_1']['mae'],
                    'metric_name': 'MAE'
                }
            if model_name == 'CAT':
                results = model.get_evals_result()
                return {
                    'train_metric': results['learn']['MAE'],
                    'val_metric': results['validation_1']['MAE'],
                    'metric_name': 'MAE'
                }
        except (AttributeError, KeyError, IndexError) as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ, ì–´ë–¤ ì˜¤ë¥˜ì¸ì§€ì™€ í•¨ê»˜ ë””ë²„ê¹… ì •ë³´ë¥¼ ìƒì„¸íˆ ì¶œë ¥í•©ë‹ˆë‹¤.
            print(f"DEBUG: Could not get history for {model_name}. Error: {e}")

            # ì¶”ê°€ ë””ë²„ê¹… ì •ë³´: ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ ëª©ë¡ì„ ì¶œë ¥ ì‹œë„
            if hasattr(model, 'evals_result_'):  # LightGBM
                print(f"DEBUG: Available keys in evals_result_: {model.evals_result_.keys()}")
            elif hasattr(model, 'get_evals_result'):  # CatBoost
                print(f"DEBUG: Available keys in get_evals_result: {model.get_evals_result().keys()}")

            return None

    class FTTWrapper:
        """
        FT-Transformer í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
        """

        def __init__(self, model_params, device='cuda'):
            self.model = None
            self.model_params = model_params
            self.device = torch.device(device if torch.cuda.is_available() else "cpu")
            self.preprocessor_ = {}
            self.history_ = {}
            self.selector_ = None

        def fit(self, X_train, y_train, X_val, y_val, feature_names, cat_threshold, best_model_save_path,
                do_variance_threshold, variance_threshold_val, do_standard_scaler):
            """
            ë¶„í• ëœ í›ˆë ¨/ê²€ì¦ ë°ì´í„°ë¥¼ ë°›ì•„ ì „ì²˜ë¦¬, ëª¨ë¸ í›ˆë ¨, ìµœê³  ëª¨ë¸ ì €ì¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
            """
            if do_variance_threshold:
                self.selector_ = VarianceThreshold(threshold=variance_threshold_val)
                X_train = self.selector_.fit_transform(X_train)
                X_val = self.selector_.transform(X_val)
                # íŠ¹ì„± ì„ íƒ í›„, feature_names ë¦¬ìŠ¤íŠ¸ë„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                feature_names = [name for name, keep in zip(feature_names, self.selector_.get_support()) if keep]

            # --- 1. ë°ì´í„° ì „ì²˜ë¦¬ (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ fit) ---
            X_train_df = pd.DataFrame(X_train, columns=feature_names)
            cat_cols = [c for c in X_train_df.columns if X_train_df[c].astype(str).nunique() <= cat_threshold]
            cont_cols = [c for c in X_train_df.columns if c not in cat_cols]

            encoders = {c: LabelEncoder().fit(X_train_df[c].astype(str)) for c in cat_cols}
            for c, enc in encoders.items():
                X_train_df[c] = enc.transform(X_train_df[c].astype(str))

            scaler = None
            if do_standard_scaler and cont_cols:
                scaler = StandardScaler()
                X_train_df[cont_cols] = scaler.fit_transform(X_train_df[cont_cols])

            # í›ˆë ¨ëœ ì „ì²˜ë¦¬ê¸°ë¥¼ í´ë˜ìŠ¤ ì†ì„±ìœ¼ë¡œ ì €ì¥
            self.preprocessor_['cat_cols'] = cat_cols
            self.preprocessor_['cont_cols'] = cont_cols
            self.preprocessor_['encoders'] = encoders
            self.preprocessor_['scaler'] = scaler

            # í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì— ì „ì²˜ë¦¬ ì ìš©
            X_val_df = pd.DataFrame(X_val, columns=feature_names)
            for c, enc in encoders.items():
                X_val_df[c] = X_val_df[c].astype(str).apply(lambda x: x if x in enc.classes_ else 'unknown')
                if 'unknown' not in enc.classes_: enc.classes_ = np.append(enc.classes_, 'unknown')
                X_val_df[c] = enc.transform(X_val_df[c])

            if scaler and cont_cols:
                X_val_df[cont_cols] = scaler.transform(X_val_df[cont_cols])

            # ìµœì¢… ë°ì´í„° ì¤€ë¹„
            X_cat_train = X_train_df[cat_cols].values if cat_cols else np.empty((len(X_train_df), 0))
            X_cont_train = X_train_df[cont_cols].values if cont_cols else np.empty((len(X_train_df), 0))
            X_cat_val = X_val_df[cat_cols].values if cat_cols else np.empty((len(X_val_df), 0))
            X_cont_val = X_val_df[cont_cols].values if cont_cols else np.empty((len(X_val_df), 0))
            categories = [len(e.classes_) for e in encoders.values()]

            # --- 2. ëª¨ë¸ í›ˆë ¨ ---
            self.model = get_model('FTT', self.model_params, categories=categories, num_continuous=len(cont_cols))
            self.model.to(self.device)

            train_ds = TabularDataset(X_cat_train, X_cont_train, y_train)
            val_ds = TabularDataset(X_cat_val, X_cont_val, y_val)
            train_loader = DataLoader(train_ds, batch_size=self.model_params['batch_size'], shuffle=True)
            val_loader = DataLoader(val_ds, batch_size=self.model_params['batch_size'])

            optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.model_params['learning_rate'],
                                          weight_decay=self.model_params['weight_decay'])
            criterion = nn.L1Loss()

            best_mae, patience = float('inf'), 0
            train_mae_history, val_mae_history = [], []

            best_model_save_path.parent.mkdir(parents=True, exist_ok=True)

            epoch_pbar = tqdm(range(self.model_params['epochs']), desc=f"Training FTT Epochs")
            for epoch in epoch_pbar:
                self.model.train()
                train_preds_for_epoch, train_true_for_epoch = [], []
                for cat, cont, target_y in train_loader:
                    optimizer.zero_grad()
                    out = self.model(cat.to(self.device), cont.to(self.device))
                    loss = criterion(out, target_y.to(self.device))
                    loss.backward()
                    clip_val = self.model_params.get('gradient_clip_val', None)
                    if clip_val is not None:
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=clip_val)
                    optimizer.step()
                    train_preds_for_epoch.extend(out.detach().cpu().numpy())
                    train_true_for_epoch.extend(target_y.cpu().numpy())
                train_mae_history.append(mean_absolute_error(train_true_for_epoch, train_preds_for_epoch))

                self.model.eval()
                val_preds_list = []
                with torch.no_grad():
                    for cat, cont, _ in val_loader:
                        val_preds_list.extend(self.model(cat.to(self.device), cont.to(self.device)).cpu().numpy())
                curr_val_mae = mean_absolute_error(y_val, val_preds_list)
                val_mae_history.append(curr_val_mae)

                if curr_val_mae < best_mae:
                    best_mae = curr_val_mae
                    patience = 0
                    torch.save(self.model.state_dict(), best_model_path)
                else:
                    patience += 1
                if patience >= self.model_params['early_stopping_patience']:
                    break

            self.history_ = {'train_mae': train_mae_history, 'val_mae': val_mae_history}

            # í›ˆë ¨ì´ ëë‚˜ë©´ ìµœê³  ì„±ëŠ¥ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œ
            self.model.load_state_dict(torch.load(best_model_path))
            return self

        def predict(self, X, feature_names):
            """
            ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë°›ì•„ ì €ì¥ëœ ì „ì²˜ë¦¬ ê·œì¹™ìœ¼ë¡œ ë³€í™˜ í›„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

            :param X: ì˜ˆì¸¡í•  ì›ë³¸ íŠ¹ì„± ë°ì´í„° (numpy array)
            :param feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            :return: ì˜ˆì¸¡ ê²°ê³¼ (numpy array)
            """
            if not self.preprocessor_ or not self.model:
                raise RuntimeError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. fit()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

            if self.selector_:
                X = self.selector_.transform(X)
                # íŠ¹ì„± ì´ë¦„ë„ ë™ì¼í•œ ê·œì¹™ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                feature_names = [name for name, keep in zip(feature_names, self.selector_.get_support()) if keep]

            # --- 1. ì €ì¥ëœ ì „ì²˜ë¦¬ê¸°ë¡œ ë°ì´í„° ë³€í™˜ ---
            X_df = pd.DataFrame(X, columns=feature_names)
            cat_cols = self.preprocessor_['cat_cols']
            cont_cols = self.preprocessor_['cont_cols']

            for c, enc in self.preprocessor_['encoders'].items():
                known_classes = set(enc.classes_)
                # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
                test_values_str = X_df[c].astype(str)
                # í›ˆë ¨ ì‹œ ë³´ì§€ ëª»í–ˆë˜ ìƒˆë¡œìš´ ê°’ì€ 'unknown'ìœ¼ë¡œ ì²˜ë¦¬
                X_df[c] = test_values_str.apply(lambda x: x if x in known_classes else 'unknown')
                # 'unknown'ì´ LabelEncoderì— ì—†ë‹¤ë©´ ì¶”ê°€
                if 'unknown' not in known_classes:
                    enc.classes_ = np.append(enc.classes_, 'unknown')
                X_df[c] = enc.transform(X_df[c])

            if self.preprocessor_['scaler'] and cont_cols:
                X_df[cont_cols] = self.preprocessor_['scaler'].transform(X_df[cont_cols])

            X_cat_test = X_df[cat_cols].values if cat_cols else np.empty((len(X_df), 0))
            X_cont_test = X_df[cont_cols].values if cont_cols else np.empty((len(X_df), 0))

            # --- 2. ì˜ˆì¸¡ ìˆ˜í–‰ ---
            self.model.eval()
            test_ds = TabularDataset(X_cat_test, X_cont_test)
            test_loader = DataLoader(test_ds, batch_size=self.model_params['batch_size'], shuffle=False)

            preds_list = []
            with torch.no_grad():
                for cat, cont in test_loader:
                    preds_list.extend(self.model(cat.to(self.device), cont.to(self.device)).cpu().numpy())

            return np.array(preds_list).flatten()

    def augment_dataset_gmm(X, y, n_samples=GMM_SAMPLES, n_components=GMM_COMPONENTS, random_state=GMM_RANDOM_STATE):
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X)
        elif not isinstance(X, pd.DataFrame):
            raise ValueError("X must be a pandas DataFrame or a NumPy array")

        # ì»¬ëŸ¼ëª…ì„ ì „ë¶€ ë¬¸ìì—´ë¡œ ë³€í™˜
        X.columns = X.columns.astype(str)

        if isinstance(y, np.ndarray):
            y = pd.Series(y)
        elif not isinstance(y, pd.Series):
            raise ValueError("y must be a pandas Series or a NumPy array")

        # íƒ€ê²Ÿì„ ì¶”ê°€í•˜ê³  í•™ìŠµ
        df = X.copy()
        df['Target'] = y.values

        # GMM í•™ìŠµ ë° ìƒ˜í”Œë§
        gmm = GaussianMixture(n_components=n_components, random_state=random_state)
        gmm.fit(df)

        synthetic_data, _ = gmm.sample(n_samples)
        synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)

        # ì›ë³¸ + í•©ì„± í•©ì¹˜ê¸°
        augmented_df = pd.concat([df, synthetic_df], ignore_index=True)

        X_augmented = augmented_df.drop(columns='Target')
        y_augmented = augmented_df['Target']

        return X_augmented, y_augmented

    # ------------------------------------------------------------
    # ğŸ“Š ì‹œê°í™” í•¨ìˆ˜
    # ------------------------------------------------------------

    def visualize_fold_results(y_val, preds, target, model_name, fold, history=None, show_plot=False):
        """ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì‹œê°í™”í•˜ê³  ê·¸ë˜í”„ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        (ET ë° ê¸°íƒ€ íŠ¸ë¦¬ ëª¨ë¸ì€ STEP ê°„ê²©ìœ¼ë¡œ í•™ìŠµ ê³¡ì„ ì„ í‘œì‹œ)
        """
        residuals = y_val - preds
        has_history = history and ('train_metric' in history or 'train_mae' in history)

        # ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ 2x2 ë˜ëŠ” 1x2 ë ˆì´ì•„ì›ƒì„ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •
        fig, axes = plt.subplots(2, 2, figsize=(14, 10)) if has_history else plt.subplots(1, 2, figsize=(14, 5))
        axes = np.ravel(axes)

        fig.suptitle(f"Results for {target} - {model_name} (Fold {fold + 1})", fontsize=16)

        # 1. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‚°ì ë„ (axes[0])
        axes[0].scatter(y_val, preds, alpha=0.6)
        min_val, max_val = min(y_val.min(), preds.min()), max(y_val.max(), preds.max())
        axes[0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Fit')
        axes[0].set(xlabel=f'Actual {target}', ylabel=f'Predicted {target}', title='Actual vs. Predicted')
        axes[0].grid(True, alpha=0.5);
        axes[0].legend()

        # 2. ì”ì°¨ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ (axes[1])
        axes[1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')
        axes[1].axvline(residuals.mean(), color='r', ls='--', lw=2, label=f"Mean: {residuals.mean():.2f}")
        axes[1].set(xlabel='Residuals (Actual - Predicted)', ylabel='Frequency', title='Residuals Distribution')
        axes[1].grid(True, alpha=0.5);
        axes[1].legend()

        # 3. í•™ìŠµ ê³¡ì„  ì‹œê°í™” (axes[2], axes[3])
        if has_history:
            metric_name = history.get('metric_name', 'Metric').upper()
            tree_models = ['LGBM', 'XGB', 'CAT']

            # --- ET ëª¨ë¸ì„ ìœ„í•œ ë¶„ë¦¬í˜• í•™ìŠµ ê³¡ì„  ---
            if model_name == 'ET':
                ax2, ax3 = axes[2], axes[3]
                x_axis_values = np.arange(1, len(history['train_metric']) + 1) * STEP

                # ì™¼ìª½ ì•„ë˜: Validation MAE ê³¡ì„ 
                ax2.plot(x_axis_values, history['val_metric'], '-o', label=f'Validation {metric_name}', markersize=4,
                         color='tab:orange')
                ax2.set_title('Validation MAE Curve')
                ax2.set_xlabel('Number of Estimators')
                ax2.set_ylabel(metric_name)
                ax2.grid(True, alpha=0.5)
                ax2.legend()

                # ì˜¤ë¥¸ìª½ ì•„ë˜: Train MAE ê³¡ì„ 
                ax3.plot(x_axis_values, history['train_metric'], '-o', label=f'Train {metric_name}', markersize=4,
                         color='tab:blue')
                ax3.set_title('Performance on Train Set (MAE)')
                ax3.set_xlabel('Number of Estimators')
                ax3.set_ylabel(metric_name)
                ax3.grid(True, alpha=0.5)
                ax3.legend()

            # --- ê·¸ ì™¸ ëª¨ë¸ì„ ìœ„í•œ í†µí•©í˜• í•™ìŠµ ê³¡ì„  ---
            else:
                ax2 = axes[2]
                # FTT ëª¨ë¸ (ëª¨ë“  Epoch í‘œì‹œ)
                if 'train_mae' in history:
                    epochs = range(1, len(history['train_mae']) + 1)
                    ax2.plot(epochs, history['train_mae'], '-o', label='Train MAE', markersize=4)
                    ax2.plot(epochs, history['val_mae'], '-o', label='Validation MAE', markersize=4)
                    ax2.set(xlabel='Epoch', ylabel='MAE', title='Train vs. Validation MAE Curve')

                # ë‹¤ë¥¸ íŠ¸ë¦¬ ëª¨ë¸ (LGBM, XGB, CAT)
                elif model_name in tree_models:
                    # STEP ê°„ê²©ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
                    train_metric_stepped = history['train_metric'][::STEP]
                    val_metric_stepped = history['val_metric'][::STEP]

                    # ìƒ˜í”Œë§ëœ ë°ì´í„°ì— ë§ëŠ” xì¶• ìƒì„± (ì‹¤ì œ ì´í„°ë ˆì´ì…˜ ë²ˆí˜¸ ë°˜ì˜)
                    x_axis_stepped = (np.arange(len(train_metric_stepped)) + 1) * STEP

                    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
                    ax2.plot(x_axis_stepped, train_metric_stepped, '-o', label=f'Train {metric_name}', markersize=4)
                    ax2.plot(x_axis_stepped, val_metric_stepped, '-o', label=f'Validation {metric_name}', markersize=4)
                    ax2.set(xlabel='Iteration', ylabel=metric_name, title=f'Train vs. Validation Curve (Step={STEP})')

                ax2.grid(True, alpha=0.5)
                ax2.legend()
                # ë„¤ ë²ˆì§¸ ì„œë¸Œí”Œë¡¯ì€ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¹„ì›Œë‘ 
                if len(axes) > 3:
                    axes[3].axis('off')

        # ì „ì²´ ë ˆì´ì•„ì›ƒ ìë™ ì¡°ì • ë° ì €ì¥
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        save_path = FOLD_GRAPH_SAVE_DIR / f"{target}_{model_name}_fold{fold + 1}_graphs.png"
        plt.savefig(save_path)
        if show_plot:
            plt.show()
        plt.close(fig)
        print(f"âœ… Visualization for Fold {fold + 1} saved to: {save_path}")

    def visualize_overall_results(y_true, y_pred, train_histories, val_histories, target, model_name, step_size=1,
                                  show_plot=False):
        """
        OOF ì˜ˆì¸¡ ê²°ê³¼ì™€ ì „ì²´ í•™ìŠµ ê³¡ì„ ì„ í•˜ë‚˜ì˜ Figureì— ì¢…í•©í•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤.
        - ìƒë‹¨: OOF ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’, OOF ì”ì°¨ ë¶„í¬
        - í•˜ë‹¨: ì „ì²´ Foldì˜ í‰ê·  í•™ìŠµ ê³¡ì„  (ET/LGBM/XGB/CAT ëª¨ë¸ì€ STEP ì ìš©, FTTëŠ” ê·¸ëŒ€ë¡œ)
        """
        oof_mae = mean_absolute_error(y_true, y_pred)

        # 2x2 ì„œë¸Œí”Œë¡¯ ë ˆì´ì•„ì›ƒ ìƒì„±
        plt.style.use('seaborn-v0_8-whitegrid')
        fig, axes = plt.subplots(2, 2, figsize=(16, 14))
        fig.suptitle(f'Overall Summary for {target} - {model_name} | OOF MAE: {oof_mae:.5f}', fontsize=20)

        # --- ìƒë‹¨: OOF ê²°ê³¼ ì‹œê°í™” ---
        # 1. (ìƒë‹¨ ì¢Œì¸¡) ì‹¤ì œê°’ vs OOF ì˜ˆì¸¡ê°’
        ax_oof_scatter = axes[0, 0]
        residuals = y_true - y_pred
        ax_oof_scatter.scatter(y_true, y_pred, alpha=0.5, s=15, edgecolors='k', linewidths=0.5)
        min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())
        ax_oof_scatter.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Fit')
        ax_oof_scatter.set_title('Actual vs. OOF Predicted', fontsize=14)
        ax_oof_scatter.set_xlabel(f'Actual {target}', fontsize=12)
        ax_oof_scatter.set_ylabel(f'OOF Predicted {target}', fontsize=12)
        ax_oof_scatter.grid(True, alpha=0.5)
        ax_oof_scatter.legend()

        # 2. (ìƒë‹¨ ìš°ì¸¡) OOF ì”ì°¨ ë¶„í¬
        ax_oof_hist = axes[0, 1]
        ax_oof_hist.hist(residuals, bins=50, alpha=0.7, edgecolor='black')
        ax_oof_hist.axvline(residuals.mean(), color='r', ls='--', lw=2, label=f"Mean: {residuals.mean():.2f}")
        ax_oof_hist.set_title('OOF Residuals Distribution', fontsize=14)
        ax_oof_hist.set_xlabel('Residuals (Actual - Predicted)', fontsize=12)
        ax_oof_hist.set_ylabel('Frequency', fontsize=12)
        ax_oof_hist.grid(True, alpha=0.5)
        ax_oof_hist.legend()

        # --- í•˜ë‹¨: ì „ì²´ í•™ìŠµ ê³¡ì„  ì‹œê°í™” ---
        try:
            min_len = min(len(h) for h in train_histories)
            train_histories_padded = [h[:min_len] for h in train_histories]
            val_histories_padded = [h[:min_len] for h in val_histories]

            mean_train_mae = np.mean(train_histories_padded, axis=0)
            std_train_mae = np.std(train_histories_padded, axis=0)
            mean_val_mae = np.mean(val_histories_padded, axis=0)
            std_val_mae = np.std(val_histories_padded, axis=0)

            # ëª¨ë¸ ì¢…ë¥˜ì— ë”°ë¼ í•™ìŠµ ê³¡ì„  í‘œì‹œ ë°©ë²•ì„ ë¶„ê¸°í•©ë‹ˆë‹¤.
            tree_models_with_step = ['LGBM', 'XGB', 'CAT']  # ETëŠ” ì•„ë˜ì—ì„œ ë³„ë„ ì²˜ë¦¬

            # 1. ET ëª¨ë¸: Train/Validation ê³¡ì„ ì„ ë¶„ë¦¬í•˜ì—¬ í‘œì‹œ
            if model_name == 'ET':
                x_axis = np.arange(1, len(mean_val_mae) + 1) * step_size

                # (í•˜ë‹¨ ì¢Œì¸¡) Validation ê³¡ì„ 
                ax_val_curve = axes[1, 0]
                ax_val_curve.plot(x_axis, mean_val_mae, 'o-', color='tab:orange', label='Average Validation MAE',
                                  markersize=4)
                ax_val_curve.fill_between(x_axis, mean_val_mae - std_val_mae, mean_val_mae + std_val_mae,
                                          color='tab:orange', alpha=0.15)
                best_idx = np.argmin(mean_val_mae)
                ax_val_curve.scatter(x_axis[best_idx], mean_val_mae[best_idx], color='red', s=100, zorder=5,
                                     label=f'Best Val MAE: {mean_val_mae[best_idx]:.5f}')
                ax_val_curve.set_title(f'Overall Validation Curve (Avg over {len(val_histories)} Folds)', fontsize=14)
                ax_val_curve.set_xlabel('Number of Estimators', fontsize=12)
                ax_val_curve.set_ylabel('Mean Absolute Error (MAE)', fontsize=12)
                ax_val_curve.legend()
                ax_val_curve.grid(True, alpha=0.5)

                # (í•˜ë‹¨ ìš°ì¸¡) Train ê³¡ì„ 
                ax_train_curve = axes[1, 1]
                ax_train_curve.plot(x_axis, mean_train_mae, 'o-', color='tab:blue', label='Average Train MAE',
                                    markersize=4)
                ax_train_curve.fill_between(x_axis, mean_train_mae - std_train_mae, mean_train_mae + std_train_mae,
                                            color='tab:blue', alpha=0.15)
                ax_train_curve.set_title(f'Overall Train Performance (Avg over {len(train_histories)} Folds)',
                                         fontsize=14)
                ax_train_curve.set_xlabel('Number of Estimators', fontsize=12)
                ax_train_curve.set_ylabel('Mean Absolute Error (MAE)', fontsize=12)
                ax_train_curve.legend()
                ax_train_curve.grid(True, alpha=0.5)

            # 2. ê·¸ ì™¸ íŠ¸ë¦¬ ëª¨ë¸ (LGBM, XGB, CAT): STEP ê°„ê²©ìœ¼ë¡œ ì ì„ ì°ì–´ í†µí•© í‘œì‹œ
            elif model_name in tree_models_with_step:
                ax_lr_curve = axes[1, 0]

                # STEP ê°„ê²©ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ìš´ìƒ˜í”Œë§
                plot_train_mae = mean_train_mae[::step_size]
                plot_val_mae = mean_val_mae[::step_size]
                plot_std_train = std_train_mae[::step_size]
                plot_std_val = std_val_mae[::step_size]

                # ìƒ˜í”Œë§ëœ ë°ì´í„°ì— ë§ëŠ” xì¶• ìƒì„± (ì‹¤ì œ ì´í„°ë ˆì´ì…˜ ë²ˆí˜¸ ë°˜ì˜)
                plot_x_axis = np.arange(1, len(plot_train_mae) + 1) * step_size

                # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
                ax_lr_curve.plot(plot_x_axis, plot_train_mae, 'o-', color='tab:blue', label='Average Train MAE',
                                 markersize=4)
                ax_lr_curve.plot(plot_x_axis, plot_val_mae, 'o-', color='tab:orange', label='Average Validation MAE',
                                 markersize=4)
                ax_lr_curve.fill_between(plot_x_axis, plot_train_mae - plot_std_train, plot_train_mae + plot_std_train,
                                         color='tab:blue', alpha=0.15)
                ax_lr_curve.fill_between(plot_x_axis, plot_val_mae - plot_std_val, plot_val_mae + plot_std_val,
                                         color='tab:orange', alpha=0.15)

                # Best MAE ì§€ì  í‘œì‹œ (ìƒ˜í”Œë§ëœ ë°ì´í„° ê¸°ì¤€)
                best_train_idx = np.argmin(plot_train_mae);
                best_val_idx = np.argmin(plot_val_mae)
                ax_lr_curve.scatter(plot_x_axis[best_train_idx], plot_train_mae[best_train_idx], color='blue', s=100,
                                    zorder=5, edgecolor='black',
                                    label=f'Best Train MAE: {plot_train_mae[best_train_idx]:.5f}')
                ax_lr_curve.scatter(plot_x_axis[best_val_idx], plot_val_mae[best_val_idx], color='darkorange', s=100,
                                    zorder=5, edgecolor='black',
                                    label=f'Best Val MAE: {plot_val_mae[best_val_idx]:.5f}')

                ax_lr_curve.set_title(f'Overall Learning Curve (Avg over {len(train_histories)} Folds)', fontsize=14)
                ax_lr_curve.set_xlabel('Iteration', fontsize=12)
                ax_lr_curve.set_ylabel('Mean Absolute Error (MAE)', fontsize=12)
                ax_lr_curve.legend(fontsize='small')
                ax_lr_curve.grid(True, which='both', linestyle='--', linewidth=0.5)
                axes[1, 1].axis('off')

            # 3. FTT ëª¨ë¸: ëª¨ë“  Epochì„ í‘œì‹œ
            else:
                ax_lr_curve = axes[1, 0]
                x_axis = np.arange(1, min_len + 1)

                # (FTT ëª¨ë¸ì˜ ê¸°ì¡´ ê·¸ë˜í”„ ë¡œì§ê³¼ ë™ì¼)
                ax_lr_curve.plot(x_axis, mean_train_mae, 'o-', color='tab:blue', label='Average Train MAE',
                                 markersize=4)
                ax_lr_curve.plot(x_axis, mean_val_mae, 'o-', color='tab:orange', label='Average Validation MAE',
                                 markersize=4)
                ax_lr_curve.fill_between(x_axis, mean_train_mae - std_train_mae, mean_train_mae + std_train_mae,
                                         color='tab:blue', alpha=0.15)
                ax_lr_curve.fill_between(x_axis, mean_val_mae - std_val_mae, mean_val_mae + std_val_mae,
                                         color='tab:orange',
                                         alpha=0.15)

                best_train_idx = np.argmin(mean_train_mae);
                best_val_idx = np.argmin(mean_val_mae)
                ax_lr_curve.scatter(x_axis[best_train_idx], mean_train_mae[best_train_idx], color='blue', s=100,
                                    zorder=5,
                                    edgecolor='black', label=f'Best Train MAE: {mean_train_mae[best_train_idx]:.5f}')
                ax_lr_curve.scatter(x_axis[best_val_idx], mean_val_mae[best_val_idx], color='darkorange', s=100,
                                    zorder=5,
                                    edgecolor='black', label=f'Best Val MAE: {mean_val_mae[best_val_idx]:.5f}')

                ax_lr_curve.set_title(f'Overall Learning Curve (Avg over {len(train_histories)} Folds)', fontsize=14)
                ax_lr_curve.set_xlabel('Epoch', fontsize=12)
                ax_lr_curve.set_ylabel('Mean Absolute Error (MAE)', fontsize=12)
                ax_lr_curve.legend(fontsize='small')
                ax_lr_curve.grid(True, which='both', linestyle='--', linewidth=0.5)
                axes[1, 1].axis('off')

        except ValueError:
            axes[1, 0].text(0.5, 0.5, 'No history data available.', ha='center', va='center')
            axes[1, 1].axis('off')
            print("âš ï¸ í•™ìŠµ ê¸°ë¡ì´ ì—†ì–´ ì „ì²´ í•™ìŠµ ê³¡ì„  ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

        # ì „ì²´ ë ˆì´ì•„ì›ƒ ì¡°ì • ë° ì €ì¥
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        save_path = GRAPH_SAVE_DIR / f"{target}_{model_name}_overall_summary_graphs.png"
        plt.savefig(save_path)
        if show_plot:
            plt.show()
        plt.close(fig)
        print(f"âœ… Overall summary visualization for '{model_name}' saved to: {save_path}")

    # ------------------------------------------------------------
    # ğŸš€ í›ˆë ¨ ëª¨ë“œ
    # ------------------------------------------------------------
    MODEL_PIPELINE_MODELS = ['ET', 'LGBM', 'XGB', 'CAT']

    if RUN_MODEL_TRAINING:
        # --- í›ˆë ¨ ë° ê²€ì¦ ëª¨ë“œ ---
        print("Starting in [Training & Validation Mode]...")

        overall_oof_scores = defaultdict(dict)

        # 1. ë©”ì¸ ë£¨í”„: íƒ€ê²Ÿë³„ í›ˆë ¨
        for target in TARGETS:
            models_to_train = MODEL_CONFIG.get(target, [])
            if not isinstance(models_to_train, list):
                models_to_train = [models_to_train]
            if not models_to_train:
                continue

            X_path, y_path = INPUT_DIR / f"X_train_{target}.npy", INPUT_DIR / f"y_train_{target}.npy"
            if not (X_path.exists() and y_path.exists()):
                print(f"âš ï¸ Files not found for target '{target}'. Skipping.");
                continue
            X, y = np.load(X_path), np.load(y_path)

            # ì›ë³¸ feature_namesë¥¼ ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ë¶ˆí•„ìš”í•œ ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
            original_feature_names = [f'f_{i}' for i in range(X.shape[1])]
            feature_map_path = INPUT_DIR / f"all_feature_names_{target}.pkl"
            if feature_map_path.exists():
                try:
                    original_feature_names = joblib.load(feature_map_path)
                    print(f"INFO: Successfully loaded original feature names for target '{target}'.")
                except Exception as e:
                    print(f"WARNING: Could not load feature names for '{target}'. Using default names. Error: {e}")
            else:
                print(f"INFO: Feature name map not found for '{target}'. Using default 'f_num' names.")

            # 2. ì¤‘ì²© ë£¨í”„: ëª¨ë¸ë³„ í›ˆë ¨
            for model_name in models_to_train:
                try:
                    print(f"\n{'=' * 60}\nğŸ¯ Training Target: '{target}' using Model: '{model_name}'\n{'=' * 60}")

                    oof_preds, fold_scores, fold_importances = np.zeros(len(y)), [], []
                    all_train_histories, all_val_histories = [], []

                    splits = list(KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE).split(X, y))

                    # 3. Fold ë£¨í”„: êµì°¨ ê²€ì¦
                    for fold, (train_idx, val_idx) in enumerate(
                            tqdm(splits, total=len(splits), desc=f"CV for {target}-{model_name}")):
                        history, y_val = None, y[val_idx]
                        X_train, y_train, X_val = X[train_idx], y[train_idx], X[val_idx]

                        # 1. ì•ˆì „í•œ ë°ì´í„° ë²”ìœ„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤ (32ë¹„íŠ¸ ì‹¤ìˆ˜ ê¸°ì¤€).
                        finfo = np.finfo(np.float32)

                        # 2. NaNê³¼ ë¬´í•œëŒ€(inf) ê°’ì„ ì•ˆì „í•œ ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                        X_train = np.nan_to_num(X_train, nan=0.0, posinf=finfo.max, neginf=finfo.min)
                        X_val = np.nan_to_num(X_val, nan=0.0, posinf=finfo.max, neginf=finfo.min)

                        # 3. "ë„ˆë¬´ í° ê°’"ì„ í¬í•¨í•œ ëª¨ë“  ê°’ì„ ì•ˆì „í•œ ë²”ìœ„ ë‚´ë¡œ ê°•ì œ ì œí•œ(clip)í•©ë‹ˆë‹¤.
                        X_train = np.clip(X_train, finfo.min, finfo.max)
                        X_val = np.clip(X_val, finfo.min, finfo.max)

                        # í˜„ì¬ Foldì—ì„œ ì‚¬ìš©í•  feature_namesë¥¼ ì›ë³¸ì—ì„œ ë³µì‚¬í•˜ì—¬ ì˜¤ì—¼ì„ ë°©ì§€í•©ë‹ˆë‹¤.
                        current_feature_names = original_feature_names.copy()

                        # GMM ì¦ê°•
                        if DO_GMM_AUGMENT:
                            # DataFrame ìƒì„± ì‹œ, ì˜¤ì—¼ë˜ì§€ ì•Šì€ current_feature_namesë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                            X_train_df = pd.DataFrame(X_train, columns=current_feature_names)
                            X_train_df, y_train_series = augment_dataset_gmm(
                                X_train_df, pd.Series(y_train),
                                n_samples=GMM_SAMPLES, n_components=GMM_COMPONENTS, random_state=GMM_RANDOM_STATE
                            )
                            # ì¦ê°• í›„, í˜„ì¬ Foldì˜ ë³€ìˆ˜ë“¤ë§Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                            current_feature_names = X_train_df.columns.tolist()
                            X_train = X_train_df.values
                            y_train = y_train_series.values

                        # --- ëª¨ë¸ ì¢…ë¥˜ì— ë”°ë¥¸ ë¶„ê¸° ---
                        if model_name == 'FTT':
                            ftt_pipeline = FTTWrapper(model_params=HPARAMS['FTT'], device=DEVICE)
                            best_model_path = FTT_DETAIL_SAVE_DIR / f'best_model_{target}_{model_name}_fold{fold}.pt'
                            ftt_pipeline.fit(
                                X_train, y_train, X_val, y_val,
                                feature_names=current_feature_names,
                                cat_threshold=CATEGORICAL_THRESHOLD,
                                best_model_save_path=best_model_path,
                                do_variance_threshold=DO_VARIANCE_THRESHOLD,
                                variance_threshold_val=VARIANCE_THRESHOLD,
                                do_standard_scaler=DO_StandardScaler
                            )
                            pipeline_path = MODEL_SAVE_DIR / f"ftt_pipeline_{target}_{model_name}_fold{fold}.pkl"
                            joblib.dump(ftt_pipeline, pipeline_path)
                            # ì˜ˆì¸¡ ì‹œì—ëŠ” í•­ìƒ ì˜¤ì—¼ë˜ì§€ ì•Šì€ ì›ë³¸ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                            preds = ftt_pipeline.predict(X_val, feature_names=original_feature_names)
                            history = ftt_pipeline.history_

                        elif model_name in MODEL_PIPELINE_MODELS:
                            # 1. íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ë™ì  êµ¬ì„±
                            pipeline_steps = []
                            if DO_VARIANCE_THRESHOLD:
                                pipeline_steps.append(('selector', VarianceThreshold(threshold=VARIANCE_THRESHOLD)))
                            if DO_StandardScaler:
                                pipeline_steps.append(('scaler', StandardScaler()))
                            pipeline_steps.append(('model', get_model(model_name, HPARAMS[model_name])))

                            pipeline = Pipeline(pipeline_steps)

                            # --- í›ˆë ¨ ë¡œì§ì„ 2ë‹¨ê³„ë¡œ ë¶„ë¦¬ ---

                            # 2. 1ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ ì „ì²´ í›ˆë ¨ (ì œì¶œ ë° ê³µì‹ ì˜ˆì¸¡ìš©)
                            print(f"INFO: Fitting pipeline for {model_name}...")
                            pipeline.fit(X_train, y_train)
                            preds = pipeline.predict(X_val)

                            # 3. 2ë‹¨ê³„: í•™ìŠµ ê³¡ì„  ìƒì„±ì„ ìœ„í•œ ë³„ë„ í›ˆë ¨
                            history = {}  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë¹ˆ history ì´ˆê¸°í™”

                            if model_name == 'ET':
                                print(f"INFO: Generating learning curve for ET by iterative fitting...")

                                # ì‹œê°í™” ì „ìš© ì„ì‹œ íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•˜ì—¬ warm_start ì¶©ëŒ ë°©ì§€
                                diag_pipeline_steps = []
                                if DO_VARIANCE_THRESHOLD:
                                    diag_pipeline_steps.append(
                                        ('selector', VarianceThreshold(threshold=VARIANCE_THRESHOLD)))
                                if DO_StandardScaler:
                                    diag_pipeline_steps.append(('scaler', StandardScaler()))

                                et_diag_params = HPARAMS['ET'].copy()
                                et_diag_params['warm_start'] = True
                                et_diag_params.pop('oob_score', None)

                                diag_model = get_model(model_name, et_diag_params)
                                diag_pipeline_steps.append(('model', diag_model))
                                diagnostic_pipeline = Pipeline(diag_pipeline_steps)

                                train_mae_history, val_mae_history = [], []
                                n_estimators_total = HPARAMS['ET']['n_estimators']
                                estimator_range = range(STEP, n_estimators_total + 1, STEP)

                                for n_estimators in tqdm(estimator_range, desc=f"LC for {model_name}", leave=False):
                                    diagnostic_pipeline.set_params(model__n_estimators=n_estimators)
                                    diagnostic_pipeline.fit(X_train, y_train)
                                    train_pred = diagnostic_pipeline.predict(X_train)
                                    val_pred = diagnostic_pipeline.predict(X_val)
                                    train_mae_history.append(mean_absolute_error(y_train, train_pred))
                                    val_mae_history.append(mean_absolute_error(y_val, val_pred))
                                history = {'train_metric': train_mae_history, 'val_metric': val_mae_history,
                                           'metric_name': 'MAE'}

                            elif model_name in ['LGBM', 'XGB', 'CAT']:
                                # ì´ ëª¨ë¸ë“¤ì€ eval_setì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ê³¡ì„ ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
                                # ì‹œê°í™”ëŠ” ì œì¶œìš© íŒŒì´í”„ë¼ì¸ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë³„ë„ ëª¨ë¸ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                                try:
                                    print(f"INFO: Generating learning curve for {model_name} (visualization only)...")

                                    # 1. ë°ì´í„° ì „ì²˜ë¦¬: í›ˆë ¨ëœ íŒŒì´í”„ë¼ì¸ì—ì„œ ëª¨ë¸ì„ ì œì™¸í•œ ì „ì²˜ë¦¬ê¸°ë§Œ ì¶”ì¶œ
                                    preprocessor_steps = [(name, step) for name, step in pipeline.named_steps.items() if
                                                          name != 'model']

                                    if preprocessor_steps:
                                        # ì „ì²˜ë¦¬ê¸°ë§Œìœ¼ë¡œ ì„ì‹œ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ì—¬ ë°ì´í„° ë³€í™˜
                                        preprocessor = Pipeline(preprocessor_steps)
                                        X_train_transformed = preprocessor.transform(X_train)
                                        X_val_transformed = preprocessor.transform(X_val)
                                    else:
                                        # ì „ì²˜ë¦¬ ë‹¨ê³„ê°€ ì—†ëŠ” ê²½ìš° ì›ë³¸ ë°ì´í„° ì‚¬ìš©
                                        X_train_transformed = X_train
                                        X_val_transformed = X_val

                                    # 2. ì‹œê°í™”ìš© ì„ì‹œ ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
                                    diagnostic_params = HPARAMS[model_name].copy()
                                    # early_stopping_roundsëŠ” fit ë©”ì„œë“œì— ì „ë‹¬í•´ì•¼ í•˜ë¯€ë¡œ ìƒì„±ì íŒŒë¼ë¯¸í„°ì—ì„œ ì œê±°
                                    early_stopping_rounds = diagnostic_params.pop('early_stopping_rounds', 100)
                                    diagnostic_model = get_model(model_name, diagnostic_params)

                                    # eval_setê³¼ í•¨ê»˜ fitì„ í˜¸ì¶œí•˜ì—¬ í•™ìŠµ ê³¡ì„  ë°ì´í„°(history) íšë“
                                    fit_params = {
                                        'eval_set': [(X_train_transformed, y_train), (X_val_transformed, y_val)]
                                    }
                                    if 'early_stopping_rounds' in HPARAMS[model_name]:
                                        fit_params['early_stopping_rounds'] = early_stopping_rounds

                                    if model_name == 'LGBM':
                                        fit_params['eval_names'] = ['train', 'valid']

                                    elif model_name in ['XGB', 'CAT']:
                                        fit_params['verbose'] = 0

                                    diagnostic_model.fit(X_train_transformed, y_train, **fit_params)

                                    # 3. í•™ìŠµ ê¸°ë¡ ì¶”ì¶œ
                                    history = get_tree_model_history(diagnostic_model, model_name)

                                    # ì´ ë¸”ë¡ì´ ëë‚˜ë©´ ì‹œê°í™”ìš© ëª¨ë¸(diagnostic_model)ì€ ìë™ìœ¼ë¡œ íê¸°ë©ë‹ˆë‹¤.

                                except Exception as e:
                                    print(f"WARNING: Could not generate learning curve for {model_name}. Error: {e}")
                                    history = {}  # ì‹¤íŒ¨ ì‹œ historyë¥¼ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”

                            # 4. íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                            pipeline_path = MODEL_SAVE_DIR / f"pipeline_{target}_{model_name}_fold{fold}.pkl"
                            joblib.dump(pipeline, pipeline_path)

                            # 5. íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ
                            try:
                                model_step = pipeline.named_steps['model']
                                if hasattr(model_step, 'feature_importances_'):
                                    final_feature_names = current_feature_names
                                    if 'selector' in pipeline.named_steps:
                                        selector_mask = pipeline.named_steps['selector'].get_support()
                                        final_feature_names = [name for name, keep in
                                                               zip(current_feature_names, selector_mask)
                                                               if keep]
                                    importances = model_step.feature_importances_
                                    if len(final_feature_names) == len(importances):
                                        imp_df = pd.DataFrame({'feature': final_feature_names, 'importance': importances})
                                        fold_importances.append(imp_df)
                                    else:
                                        print(f"WARNING: FI length mismatch after selection.")
                            except Exception as e:
                                print(f"Could not extract feature importances: {e}")

                        # í˜„ì¬ í´ë“œì˜ í•™ìŠµ ê¸°ë¡ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                        if history:
                            if 'train_mae' in history:  # FTT case
                                all_train_histories.append(history['train_mae'])
                                all_val_histories.append(history['val_mae'])
                            elif 'train_metric' in history:  # Tree models case
                                all_train_histories.append(history['train_metric'])
                                all_val_histories.append(history['val_metric'])

                        # í´ë“œë³„ ì„±ëŠ¥ ê³„ì‚° ë° ì‹œê°í™”
                        oof_preds[val_idx], score = preds, mean_absolute_error(y_val, preds)
                        fold_scores.append(score)
                        visualize_fold_results(y_val, preds, target, model_name, fold, history=history, show_plot=False)
                    # Overall í•™ìŠµ ê³¡ì„  ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
                    if all_train_histories and all_val_histories:
                        tree_models = ['ET', 'LGBM', 'XGB', 'CAT']
                        step_size = STEP if model_name in tree_models else 1
                        visualize_overall_results(
                            y_true=y,
                            y_pred=oof_preds,
                            train_histories=all_train_histories,
                            val_histories=all_val_histories,
                            target=target,
                            model_name=model_name,
                            step_size=step_size,
                            show_plot=False  # Trueë¡œ ë°”ê¾¸ë©´ ê·¸ë˜í”„ê°€ í™”ë©´ì— í‘œì‹œë©ë‹ˆë‹¤.
                        )
                    # --- ëª¨ë¸ë³„ ê²°ê³¼ ì €ì¥ (ëª¨ë“  í´ë“œ í›ˆë ¨ ì™„ë£Œ í›„) ---
                    if fold_importances:
                        agg_imp_df = pd.concat(fold_importances).groupby('feature')['importance'].mean().sort_values(
                            ascending=False).reset_index()
                        imp_path = IMPORTANCE_SAVE_DIR / f"importance_{target}_{model_name}_aggregated.csv"
                        agg_imp_df.to_csv(imp_path, index=False)
                        print(f"âœ… Aggregated feature importance saved to: {imp_path}")

                    # ëª¨ë¸ì˜ ìµœì¢… Out-of-Fold MAE ê³„ì‚° ë° ì €ì¥
                    oof_mae = mean_absolute_error(y, oof_preds)
                    overall_oof_scores[target][model_name] = oof_mae
                    print(
                        f"\n--- Finished for '{target}' | Model: '{model_name}' | Avg Fold MAE: {np.mean(fold_scores):.5f} Â± {np.std(fold_scores):.5f} | OOF MAE: {oof_mae:.5f} ---\n")

                    # OOF ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (npy ë° ìƒì„¸ csv)
                    np.save(OOF_SAVE_DIR / f"oof_preds_{target}_{model_name}.npy", oof_preds)
                    oof_details_df = pd.DataFrame(
                        {'sample_index': np.arange(len(y)), 'y_true': y, f'y_pred_{model_name}': oof_preds,
                         'error': np.abs(y - oof_preds)}).sort_values(by='error', ascending=False)
                    oof_details_path = OOF_SAVE_DIR / f"oof_details_{target}_{model_name}.csv"
                    oof_details_df.to_csv(oof_details_path, index=False)
                    print(f"âœ… Detailed OOF results saved to: {oof_details_path}")

                except Exception as e:
                    # ğŸ”´ğŸ”´ğŸ”´ EXCEPT ë¸”ë¡: ì—ëŸ¬ ë°œìƒ ì‹œ ì‹¤í–‰ë˜ëŠ” ë¶€ë¶„ ğŸ”´ğŸ”´ğŸ”´
                    # ì–´ë–¤ ëª¨ë¸ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆëŠ”ì§€ ëª…í™•í•˜ê²Œ ë¡œê¹…
                    print(f"\n{'!' * 60}")
                    print(f"ğŸ”´ CRITICAL ERROR during training of '{model_name}' for target '{target}'.")
                    print(f"ğŸ”´ Error Type: {type(e).__name__}")
                    print(f"ğŸ”´ Error Message: {e}")
                    import traceback
                    traceback.print_exc()  # ìƒì„¸í•œ ì—ëŸ¬ ìœ„ì¹˜ ì¶”ì 
                    print(f"{'!' * 60}\n")

                    # ì‹¤íŒ¨ë¥¼ ê¸°ë¡í•˜ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°
                    overall_oof_scores[target][model_name] = np.nan  # ì‹¤íŒ¨í•œ ëª¨ë¸ì€ NaNìœ¼ë¡œ ì ìˆ˜ ê¸°ë¡
                    continue  # í˜„ì¬ ëª¨ë¸ í›ˆë ¨ì„ ì¤‘ë‹¨í•˜ê³  for ë£¨í”„ì˜ ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°

                finally:
                    # âœ…âœ…âœ… FINALLY ë¸”ë¡: ì„±ê³µí•˜ë“  ì‹¤íŒ¨í•˜ë“  í•­ìƒ ì‹¤í–‰ âœ…âœ…âœ…
                    # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
                    # ì´ë¥¼ í†µí•´ ì‹¤íŒ¨í•œ ëª¨ë¸ì´ ì ìœ í•˜ë˜ ë©”ëª¨ë¦¬ë„ ì •ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    print(f"--- Cleaning up resources for {model_name} on target {target} ---")

                    # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰
                    gc.collect()

            # ------------------------------------------------------------
            # ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½ ë° íŒŒì¼ ì €ì¥
            # ------------------------------------------------------------
            print("\n\nAll Training & Analysis Finished!")

            # 1. íŒŒì¼ì— ì €ì¥í•  ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±
            summary_lines = []
            summary_lines.append("================== Summary of OOF MAE ==================")
            for target, model_scores in overall_oof_scores.items():
                summary_lines.append(f"\n--- Target: {target} ---")
                if not model_scores:
                    summary_lines.append("  No models were trained for this target.")
                else:
                    for model_name, score in model_scores.items():
                        summary_lines.append(f"  Model '{model_name}': {score:.5f}")
            summary_lines.append("========================================================")

            # 2. ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
            summary_text = "\n".join(summary_lines)

            # 3. í™”ë©´ì— ê²°ê³¼ ì¶œë ¥ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
            print(summary_text)

            # 4. ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ì— ì €ì¥
            try:
                summary_file_path = OOF_SAVE_DIR / "oof_summary.txt"
                with open(summary_file_path, 'w', encoding='utf-8') as f:
                    f.write(summary_text)
                print(f"\nâœ… OOF MAE summary saved to: {summary_file_path}")
            except Exception as e:
                print(f"\nâš ï¸ Could not save OOF MAE summary. Error: {e}")

    # ------------------------------------------------------------
    # ğŸš€ ì œì¶œ ëª¨ë“œ
    # ------------------------------------------------------------
    MODEL_PIPELINE_MODELS = ['ET', 'LGBM', 'XGB', 'CAT']

    if not RUN_MODEL_TRAINING:
        print("RUN_MODEL_TRAINING=False. ì™¸ë¶€ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ íŒŒì¼ì„ ë³µì‚¬í•©ë‹ˆë‹¤...")

        # MODEL_SAVE_DIRë¡œ í•„ìš”í•œ ëª¨ë“  .pkl íŒŒì¼ì„ ë³µì‚¬
        # (FTT ëª¨ë¸ì˜ ê²½ìš°, FTT_DETAIL_SAVE_DIRë¡œ .pt íŒŒì¼ ë³µì‚¬)
        shutil.copytree(MODEL_INPUT_DIR, MODEL_SAVE_DIR, dirs_exist_ok=True)
        print("âœ… ëª¨ë¸ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ.")

    if SUBMISSION_MODE:
        print("\nğŸš€ Starting in [Submission Mode]...")

        if __name__ == "__main__":
            print("\nmain_test() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            main_test()

        # ìƒ˜í”Œ ì œì¶œ íŒŒì¼(template) ë¡œë“œí•´ì„œ ì»¬ëŸ¼ëª…ë§Œ ê°€ì ¸ì˜¤ê¸°
        sample_df = pd.read_csv(SAMPLE_SUBMISSION / 'sample_submission.csv')
        cols = sample_df.columns.tolist()  # ['id','Tg','FFV','Tc','Density','Rg']

        # ë¹ˆ DataFrame ìƒì„± (í–‰ ìˆ˜ëŠ” test_df ê¸°ì¤€)
        submission_df = pd.DataFrame(columns=cols, index=range(len(test_df)))

        # id ì¹¼ëŸ¼ë§Œ test_dfì˜ id ë¡œ ì±„ìš°ê¸°
        submission_df['id'] = test_df['id'].values

        # ëª¨ë“  íƒ€ê²Ÿì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
        for target in tqdm(TARGETS, desc="Predicting Targets"):
            models_to_predict = MODEL_CONFIG.get(target, [])
            if not isinstance(models_to_predict, list): models_to_predict = [models_to_predict]
            if not models_to_predict: continue

            # Test ë°ì´í„° ë¡œë“œ (main_testì—ì„œ ìƒì„±ëœ raw feature)
            X_test_path = TEST_INPUT_DIR / f"X_test_{target}.npy"
            if not X_test_path.exists():
                print(f"âš ï¸ Test data file not found for target '{target}'. Skipping.");
                continue
            X_test_raw = np.load(X_test_path)

            # íŠ¹ì„± ì´ë¦„ ë¡œë”© (FTTWrapperì˜ predict ë©”ì„œë“œì— í•„ìš”)
            feature_names = []
            feature_map_path = INPUT_DIR / f"all_feature_names_{target}.pkl"
            if feature_map_path.exists():
                try:
                    feature_names = joblib.load(feature_map_path)
                except Exception:
                    pass

            target_predictions = []

            # í•´ë‹¹ íƒ€ê²Ÿì— ëŒ€í•´ ì„¤ì •ëœ ëª¨ë“  ëª¨ë¸ë¡œ ì˜ˆì¸¡
            for model_name in models_to_predict:
                print(f"  > Predicting with {model_name} for {target}...")

                fold_predictions = []
                num_folds_to_load = N_SPLITS if N_SPLITS >= 2 else 1

                for fold in range(num_folds_to_load):

                    # ëª¨ë¸ ì¢…ë¥˜ì— ë”°ë¼ ë°ì´í„° ì²˜ë¦¬ ë°©ì‹ì„ ë¶„ê¸°
                    if model_name == 'FTT':
                        # 1. í›ˆë ¨ëœ FTTWrapper ê°ì²´ ì „ì²´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
                        pipeline_path = MODEL_SAVE_DIR / f"ftt_pipeline_{target}_{model_name}_fold{fold}.pkl"
                        ftt_pipeline = joblib.load(pipeline_path)

                        # 2. Wrapperì˜ predict ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
                        #    ë‚´ë¶€ì ìœ¼ë¡œ ì „ì²˜ë¦¬(ì¸ì½”ë”©, ìŠ¤ì¼€ì¼ë§)ê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
                        preds = ftt_pipeline.predict(X_test_raw, feature_names=feature_names)
                        fold_predictions.append(preds)

                    elif model_name in MODEL_PIPELINE_MODELS:
                        # 1. í›ˆë ¨ëœ íŒŒì´í”„ë¼ì¸ ê°ì²´ ì „ì²´ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
                        pipeline_path = MODEL_SAVE_DIR / f"pipeline_{target}_{model_name}_fold{fold}.pkl"
                        pipeline = joblib.load(pipeline_path)

                        # 2. íŒŒì´í”„ë¼ì¸ì˜ predict ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
                        #    ë‚´ë¶€ì ìœ¼ë¡œ VarianceThreshold, StandardScalerê°€ ìë™ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤.
                        preds = pipeline.predict(X_test_raw)
                        fold_predictions.append(preds)

                # ëª¨ë“  Foldì˜ ì˜ˆì¸¡ê°’ì„ í‰ê· 
                model_avg_preds = np.mean(fold_predictions, axis=0)
                target_predictions.append(model_avg_preds)

            # í•œ íƒ€ê²Ÿì— ì—¬ëŸ¬ ëª¨ë¸ì„ ì‚¬ìš©í•œ ê²½ìš°, ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ê°’ì„ ë‹¤ì‹œ í‰ê·  (ì•™ìƒë¸”)
            final_preds_for_target = np.mean(target_predictions, axis=0)
            submission_df[target] = final_preds_for_target

        # ìµœì¢… submission íŒŒì¼ ì €ì¥
        submission_path = SUBMISSION_SAVE_DIR / "submission.csv"
        submission_df.to_csv(submission_path, index=False)
        print(f"\nâœ… Submission file created successfully at: {submission_path}")

    # =================================================================

    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"\nExperiment completed in {elapsed_time:.2f} seconds.")

    # âœ… ê²°ê³¼ ìš”ì•½ì„ ìœ„í•´ dict í˜•íƒœë¡œ ë°˜í™˜
    result = params.copy()  # ì…ë ¥ íŒŒë¼ë¯¸í„°ë¥¼ ê²°ê³¼ì— í¬í•¨
    result['elapsed_time_sec'] = elapsed_time

    return result


# ëª¨ë¸ ì„¤ì • (ì‹¤í—˜ ê´€ë ¨ íŒŒë¼ë¯¸í„°)
experiment_params = [
    # Row 1
    {"DO_RDKit": True, "DO_POLYBERT": False, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": False, "N_SPLITS": 5},
    # Row 2
    {"DO_RDKit": True, "DO_POLYBERT": False, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": True, "AUG_NUM": 1, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.1, "DO_StandardScaler": False, "N_SPLITS": 10},
    # Row 3
    {"DO_RDKit": True, "DO_POLYBERT": False, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 100, "GMM_COMPONENTS": 3, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.01, "DO_StandardScaler": True, "N_SPLITS": 5},
    # Row 4
    {"DO_RDKit": True, "DO_POLYBERT": False, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": True, "AUG_NUM": 3, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 500, "GMM_COMPONENTS": 5, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": True, "N_SPLITS": 10},
    # Row 5
    {"DO_RDKit": False, "DO_POLYBERT": True, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": True, "AUG_NUM": 5, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.001, "DO_StandardScaler": False, "N_SPLITS": 5},
    # Row 6
    {"DO_RDKit": False, "DO_POLYBERT": True, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": False, "N_SPLITS": 10},
    # Row 7
    {"DO_RDKit": False, "DO_POLYBERT": True, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": True, "AUG_NUM": 1, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 1000, "GMM_COMPONENTS": 7, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": True, "N_SPLITS": 5},
    # Row 8
    {"DO_RDKit": False, "DO_POLYBERT": True, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 100, "GMM_COMPONENTS": 3, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.1, "DO_StandardScaler": True, "N_SPLITS": 10},
    # Row 9
    {"DO_RDKit": False, "DO_POLYBERT": False, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.01, "DO_StandardScaler": True, "N_SPLITS": 5},
    # Row 10
    {"DO_RDKit": False, "DO_POLYBERT": False, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": True, "AUG_NUM": 3, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": True, "N_SPLITS": 10},
    # Row 11
    {"DO_RDKit": False, "DO_POLYBERT": False, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 500, "GMM_COMPONENTS": 5, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": False, "N_SPLITS": 5},
    # Row 12
    {"DO_RDKit": False, "DO_POLYBERT": False, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": True, "AUG_NUM": 5, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 1000, "GMM_COMPONENTS": 7, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.001, "DO_StandardScaler": False, "N_SPLITS": 10},
    # Row 13
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.1, "DO_StandardScaler": True, "N_SPLITS": 5},
    # Row 14
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": True, "AUG_NUM": 1, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": True, "N_SPLITS": 10},
    # Row 15
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": True, "AUG_NUM": 3, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 100, "GMM_COMPONENTS": 3, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": False, "N_SPLITS": 5},
    # Row 16
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": False, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 500, "GMM_COMPONENTS": 5, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.01, "DO_StandardScaler": False, "N_SPLITS": 10},
    # Row 17
    {"DO_RDKit": True, "DO_POLYBERT": False, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": False, "N_SPLITS": 5},
    # Row 18
    {"DO_RDKit": True, "DO_POLYBERT": False, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": True, "AUG_NUM": 5, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.001, "DO_StandardScaler": False, "N_SPLITS": 10},
    # Row 19
    {"DO_RDKit": True, "DO_POLYBERT": False, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": True, "AUG_NUM": 1, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 1000, "GMM_COMPONENTS": 7, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.1, "DO_StandardScaler": True, "N_SPLITS": 5},
    # Row 20
    {"DO_RDKit": True, "DO_POLYBERT": False, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 100, "GMM_COMPONENTS": 3, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": True, "N_SPLITS": 10},
    # Row 21
    {"DO_RDKit": False, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.01, "DO_StandardScaler": False, "N_SPLITS": 5},
    # Row 22
    {"DO_RDKit": False, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": True, "AUG_NUM": 3, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": False, "N_SPLITS": 10},
    # Row 23
    {"DO_RDKit": False, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 500, "GMM_COMPONENTS": 5, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.001, "DO_StandardScaler": True, "N_SPLITS": 5},
    # Row 24
    {"DO_RDKit": False, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": 0, "DO_AUGMENT_SMILES": True, "AUG_NUM": 5, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 1000, "GMM_COMPONENTS": 7, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": True, "N_SPLITS": 10},
    # Row 25
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": False, "N_SPLITS": 5},
    # Row 26
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": True, "AUG_NUM": 1, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.1, "DO_StandardScaler": False, "N_SPLITS": 10},
    # Row 27
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 100, "GMM_COMPONENTS": 3, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.01, "DO_StandardScaler": True, "N_SPLITS": 5},
    # Row 28
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": True, "AUG_NUM": 3, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 500, "GMM_COMPONENTS": 5, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": True, "N_SPLITS": 10},
    # Row 29
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": True, "AUG_NUM": 5, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 1000, "GMM_COMPONENTS": 7, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": False, "N_SPLITS": 5},
    # Row 30
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": True, "GMM_SAMPLES": 100, "GMM_COMPONENTS": 3, "GMM_RANDOM_STATE": 42, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.001, "DO_StandardScaler": False, "N_SPLITS": 10},
    # Row 31
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "required", "DO_AUGMENT_SMILES": True, "AUG_NUM": 1, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": False, "VARIANCE_THRESHOLD": 0, "DO_StandardScaler": True, "N_SPLITS": 5},
    # Row 32
    {"DO_RDKit": True, "DO_POLYBERT": True, "DO_FINGERPRINT": True, "RDKit_FILTER_MODE": "useless", "DO_AUGMENT_SMILES": False, "AUG_NUM": 0, "DO_GMM_AUGMENT": False, "GMM_SAMPLES": 0, "GMM_COMPONENTS": 0, "GMM_RANDOM_STATE": 0, "DO_VARIANCE_THRESHOLD": True, "VARIANCE_THRESHOLD": 0.1, "DO_StandardScaler": True, "N_SPLITS": 10}
]

# âœ… ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
all_results = []
MAIN_OUTPUT_DIR = Path("Output")
MAIN_OUTPUT_DIR.mkdir(exist_ok=True)

# âœ… ì‹¤í—˜ì„ ë°˜ë³µí•˜ê¸° ìœ„í•œ for loop
# 1. tqdm ê°ì²´ë¥¼ ë¨¼ì € ìƒì„±í•˜ê³ , file=sys.stdout ì¸ìë¥¼ ì „ë‹¬
progress_bar = tqdm(experiment_params, desc="Running Experiments", file=sys.stdout, dynamic_ncols=True)

# 2. ì „ì²´ ë£¨í”„ë¥¼ ìƒˆë¡œ ì •ì˜í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ ê°ì‹¸
with redirect_to_tqdm():
    for idx, params in enumerate(progress_bar):
        exp_num = idx + 1
        OUTPUT_DIR = MAIN_OUTPUT_DIR / f"Output{exp_num}"
        OUTPUT_DIR.mkdir(exist_ok=True, parents=True)

        log_filename = OUTPUT_DIR / f"experiment_{exp_num}_log.txt"
        logger = None  # logger ì´ˆê¸°í™”

        try:
            # TeeLoggerëŠ” ì´ì œ redirect_to_tqdmì´ ë§Œë“¤ì–´ì¤€ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ ë™ì‘
            logger = TeeLogger(str(log_filename), mode='w')

            # âœ… í•¨ìˆ˜í™”ëœ ì‹¤í—˜ ë¡œì§ í˜¸ì¶œ
            result = run_single_experiment(params, OUTPUT_DIR)
            result['experiment_id'] = exp_num
            result['status'] = 'Success'
            all_results.append(result)

        except Exception as e:
            # âœ… ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê·¸ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
            error_message = f"!!!!!! Experiment {exp_num} FAILED !!!!!!\nError: {e}"
            print(error_message)

            # ì—ëŸ¬ ë°œìƒ ì‹œ ê²°ê³¼ ê¸°ë¡
            failed_result = params.copy()
            failed_result['experiment_id'] = exp_num
            failed_result['status'] = 'Failed'
            failed_result['error_message'] = str(e)
            all_results.append(failed_result)
            continue  # ë‹¤ìŒ ì‹¤í—˜ìœ¼ë¡œ ë„˜ì–´ê°

        finally:
            # âœ… Logger ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            if logger:
                logger.close()

            # ìì› í•´ì œ
            gc.collect()

# âœ… ëª¨ë“  ì‹¤í—˜ì´ ëë‚œ í›„, ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ CSV íŒŒì¼ë¡œ ì €ì¥
print("\n" + "=" * 50)
print("All experiments are complete. Aggregating results...")
if all_results:
    results_df = pd.DataFrame(all_results)

    # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬ (ì¤‘ìš”í•œ ì •ë³´ ì•ìœ¼ë¡œ)
    core_cols = ['experiment_id', 'status', 'elapsed_time_sec']
    param_cols = [col for col in results_df.columns if col not in core_cols]
    results_df = results_df[core_cols + param_cols]

    summary_path = MAIN_OUTPUT_DIR / "experiment_summary.csv"
    results_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"Results summary saved to: {summary_path}")
else:
    print("No results to summarize.")